"""
æ‰¹é‡ä¸‹è½½ Agents4Science 2025 ä¼šè®®çš„æ‰€æœ‰è®ºæ–‡ã€review histories å’Œä»£ç 
Batch download all papers, review histories, and code from Agents4Science 2025 conference
"""

import openreview
import requests
import os
import json
import re
import csv
from pathlib import Path
from urllib.parse import urlparse, parse_qs
import time

# é…ç½® / Configuration
# ä»£ç åœ¨codeæ–‡ä»¶å¤¹è¿è¡Œï¼Œdownloadsåœ¨çˆ¶ç›®å½•ï¼ˆæ ¹ç›®å½•ï¼‰
# Code runs in code folder, downloads in parent directory (root)
SCRIPT_DIR = Path(__file__).parent
BASE_DIR = SCRIPT_DIR.parent / "downloads"
PAPERS_DIR = BASE_DIR / "papers"
REVIEWS_DIR = BASE_DIR / "reviews"
CODE_DIR = BASE_DIR / "code"
METADATA_DIR = BASE_DIR / "metadata"

# åˆ›å»ºç›®å½• / Create directories
for dir_path in [PAPERS_DIR, REVIEWS_DIR, CODE_DIR, METADATA_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# åˆå§‹åŒ– OpenReview å®¢æˆ·ç«¯ / Initialize OpenReview client
client = openreview.api.OpenReviewClient(
    baseurl="https://api2.openreview.net"
)

def extract_table_data_from_page(url):
    """
    ä»ç½‘é¡µæå–è¡¨æ ¼æ•°æ®ï¼ˆåŒ…æ‹¬forum IDå’Œè¡¨æ ¼ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼‰
    Extract table data from webpage (including forum IDs and all table information)
    è¿”å›: dict {forum_id: {status, primary_topic, secondary_topic, human_review, ai_reviewer_1, ai_reviewer_2, ai_reviewer_3, hypothesis_development}}
    Returns: dict {forum_id: {status, primary_topic, secondary_topic, human_review, ai_reviewer_1, ai_reviewer_2, ai_reviewer_3, hypothesis_development}}
    """
    print(f"ğŸ“„ æ­£åœ¨ä»ç½‘é¡µæå–è¡¨æ ¼æ•°æ®... / Extracting table data from webpage...")
    try:
        # è®¾ç½® User-Agent æ¨¡æ‹Ÿæµè§ˆå™¨
        # Set User-Agent to simulate browser
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, timeout=30, headers=headers)
        response.raise_for_status()
        
        table_data = {}
        
        # æå–æ‰€æœ‰åŒ…å«openreviewé“¾æ¥çš„è¡Œï¼Œå¹¶å°è¯•æå–å¯¹åº”çš„è¡¨æ ¼æ•°æ®
        # Extract all rows containing openreview links and try to extract corresponding table data
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¡¨æ ¼è¡Œå’Œforum ID
        # Use regex to match table rows and forum IDs
        
        # åŒ¹é…åŒ…å«forum IDçš„é“¾æ¥
        # Match links containing forum IDs
        link_pattern = r'openreview\.net/forum\?id=([a-zA-Z0-9]+)'
        forum_ids = list(set(re.findall(link_pattern, response.text)))
        
        # å¯¹äºæ¯ä¸ªforum IDï¼Œå°è¯•æå–åŒä¸€è¡Œæˆ–é™„è¿‘è¡Œçš„è¡¨æ ¼æ•°æ®
        # For each forum ID, try to extract table data from the same row or nearby rows
        # ç”±äºç½‘é¡µæ˜¯åŠ¨æ€åŠ è½½çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ–¹æ³•ï¼šæå–æ‰€æœ‰å¯èƒ½çš„æ•°æ®æ¨¡å¼
        # Since the webpage is dynamically loaded, we use a simplified approach: extract all possible data patterns
        
        # å°è¯•æå–Statusï¼ˆAccepted/Rejectedï¼‰
        # Try to extract Status (Accepted/Rejected)
        status_pattern = r'(Accepted|Rejected)'
        
        # å°è¯•æå–Primary Topicå’ŒSecondary Topic
        # Try to extract Primary Topic and Secondary Topic
        topic_patterns = [
            r'Computer & Data Sciences',
            r'Life & Health Sciences',
            r'Natural Sciences',
            r'Engineering & Technology',
            r'Social Sciences',
            r'Artificial Intelligence & Machine Learning',
            r'Human-Computer Interaction',
            # å¯ä»¥æ·»åŠ æ›´å¤šä¸»é¢˜
            # Can add more topics
        ]
        
        print(f"âœ… æ‰¾åˆ° {len(forum_ids)} ä¸ªforum ID / Found {len(forum_ids)} forum IDs")
        print("âš ï¸  æ³¨æ„ï¼šè¡¨æ ¼æ•°æ®éœ€è¦JavaScriptæ¸²æŸ“ï¼Œå°†ä»å·²ä¿å­˜çš„æ•°æ®ä¸­è¯»å– / Note: Table data requires JavaScript rendering, will read from saved data")
        
        return forum_ids, table_data
    except Exception as e:
        print(f"âŒ ä»ç½‘é¡µæå–å¤±è´¥ / Failed to extract from webpage: {e}")
        return [], {}

def extract_forum_ids_from_page(url):
    """
    ä»ç½‘é¡µæå–æ‰€æœ‰è®ºæ–‡çš„ forum IDï¼ˆç®€åŒ–ç‰ˆï¼Œä¸»è¦ç”¨äºè·å–IDåˆ—è¡¨ï¼‰
    Extract all paper forum IDs from the webpage (simplified version, mainly for getting ID list)
    """
    forum_ids, _ = extract_table_data_from_page(url)
    return forum_ids

def get_all_papers_from_api(invitation_pattern=None):
    """
    é€šè¿‡OpenReview APIè·å–æ‰€æœ‰è®ºæ–‡ï¼ˆå¦‚æœçŸ¥é“invitation patternï¼‰
    Get all papers via OpenReview API (if invitation pattern is known)
    """
    print(f"ğŸ“¡ æ­£åœ¨é€šè¿‡APIè·å–æ‰€æœ‰è®ºæ–‡... / Getting all papers via API...")
    try:
        # å°è¯•è·å–æ‰€æœ‰submission notes
        # Try to get all submission notes
        # æ³¨æ„ï¼šéœ€è¦çŸ¥é“å…·ä½“çš„invitation pattern
        # Note: Need to know the specific invitation pattern
        
        # Agents4Science 2025 å¯èƒ½çš„invitationæ ¼å¼
        # Possible invitation format for Agents4Science 2025
        possible_invitations = [
            "Agents4Science.stanford.edu/2025/Conference/-/Submission",
            "Agents4Science.stanford.edu/2025/-/Submission",
            "Agents4Science/2025/Conference/-/Submission",
        ]
        
        for invitation in possible_invitations:
            try:
                notes = client.get_all_notes(
                    invitation=invitation,
                    details="all",
                    limit=10000
                )
                
                if notes and len(notes) > 0:
                    forum_ids = [note.forum for note in notes if hasattr(note, 'forum')]
                    forum_ids = list(set(forum_ids))
                    print(f"âœ… é€šè¿‡APIæ‰¾åˆ° {len(forum_ids)} ç¯‡è®ºæ–‡ / Found {len(forum_ids)} papers via API")
                    return forum_ids
            except:
                continue
        
        print("âš ï¸  APIæ–¹æ³•æœªæ‰¾åˆ°è®ºæ–‡ï¼Œå°†ä½¿ç”¨ç½‘é¡µæå–æ–¹æ³• / API method found no papers, will use webpage extraction")
        return []
    except Exception as e:
        print(f"âš ï¸  APIæ–¹æ³•å¤±è´¥ / API method failed: {e}")
        print("å°†ä½¿ç”¨ç½‘é¡µæå–æ–¹æ³• / Will use webpage extraction method")
        return []

def download_file(url, filepath, max_retries=3):
    """
    ä¸‹è½½æ–‡ä»¶ï¼Œå¸¦é‡è¯•æœºåˆ¶
    Download file with retry mechanism
    """
    for attempt in range(max_retries):
        try:
            response = requests.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            return True
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"  âš ï¸  é‡è¯• {attempt + 1}/{max_retries}... / Retry {attempt + 1}/{max_retries}...")
                time.sleep(2)
            else:
                print(f"  âŒ ä¸‹è½½å¤±è´¥ / Download failed: {e}")
                return False
    return False

def get_paper_details(forum_id):
    """
    è·å–è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
    Get detailed information about a paper
    """
    try:
        # è·å–è®ºæ–‡ä¸»note
        # Get main paper note
        notes = client.get_notes(forum=forum_id, details="all", limit=1000)
        
        if not notes:
            return None
        
        main_note = notes[0]  # ç¬¬ä¸€ä¸ªæ˜¯ä¸»è®ºæ–‡ / First one is the main paper
        
        # è·å–æ‰€æœ‰reviews
        # Get all reviews
        reviews = [note for note in notes if 'Review' in note.invitation or 'review' in note.invitation.lower()]
        
        # è·å–æ‰€æœ‰ä»£ç é™„ä»¶
        # Get all code attachments
        code_attachments = []
        if hasattr(main_note, 'content') and main_note.content:
            for key, value in main_note.content.items():
                if isinstance(value, str) and ('code' in key.lower() or 'github' in value.lower() or 'git' in value.lower()):
                    code_attachments.append({'key': key, 'value': value})
        
        return {
            'main_note': main_note,
            'reviews': reviews,
            'code_attachments': code_attachments,
            'all_notes': notes
        }
    except Exception as e:
        print(f"  âŒ è·å–è®ºæ–‡è¯¦æƒ…å¤±è´¥ / Failed to get paper details: {e}")
        return None

def load_table_data():
    """
    ä»ä¿å­˜çš„è¡¨æ ¼æ•°æ®æ–‡ä»¶åŠ è½½è¡¨æ ¼ä¿¡æ¯ï¼ˆæ”¯æŒ CSV æ ¼å¼ï¼‰
    Load table data from saved table data file (supports CSV format)
    æ ¼å¼: {forum_id: {title, status, primary_topic, ...}}
    Format: {forum_id: {title, status, primary_topic, ...}}
    """
    # ä¼˜å…ˆå°è¯• CSV æ ¼å¼
    # Priority: try CSV format
    table_data_path_csv = BASE_DIR / "table_data.csv"
    if table_data_path_csv.exists():
        try:
            table_data = {}
            with open(table_data_path_csv, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    forum_id = row.get('forum_id', '').strip()
                    if forum_id:
                        table_data[forum_id] = {
                            'title': row.get('title', '').strip(),
                            'status': row.get('status', '').strip(),
                            'primary_topic': row.get('primary_topic', '').strip(),
                            'secondary_topic': row.get('secondary_topic', '').strip(),
                            'human_review': row.get('human_review', '').strip(),
                            'ai_reviewer_1': row.get('ai_reviewer_1', '').strip(),
                            'ai_reviewer_2': row.get('ai_reviewer_2', '').strip(),
                            'ai_reviewer_3': row.get('ai_reviewer_3', '').strip(),
                            'hypothesis_development': row.get('hypothesis_development', '').strip(),
                        }
            if table_data:
                return table_data
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ CSV è¡¨æ ¼æ•°æ®å¤±è´¥ / Failed to load CSV table data: {e}")
    
    # å¤‡ç”¨ï¼šå°è¯• JSON æ ¼å¼ï¼ˆå‘åå…¼å®¹ï¼‰
    # Fallback: try JSON format (backward compatibility)
    table_data_path_json = BASE_DIR / "table_data.json"
    if table_data_path_json.exists():
        try:
            with open(table_data_path_json, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®ï¼šforum_id ä½œä¸º key
                # Ensure data format is correct: forum_id as key
                if isinstance(data, dict):
                    return data
                else:
                    print("âš ï¸  è¡¨æ ¼æ•°æ®æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸º {forum_id: {...}} / Table data format incorrect, should be {forum_id: {...}}")
                    return {}
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ JSON è¡¨æ ¼æ•°æ®å¤±è´¥ / Failed to load JSON table data: {e}")
            return {}
    
    return {}

def download_paper(forum_id, paper_info, table_data=None):
    """
    ä¸‹è½½å•ç¯‡è®ºæ–‡çš„æ‰€æœ‰èµ„æº
    Download all resources for a single paper
    """
    main_note = paper_info['main_note']
    title = main_note.content.get('title', 'Untitled') if hasattr(main_note, 'content') else 'Untitled'
    
    # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ / Clean illegal characters from filename
    safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)[:100]
    paper_folder = PAPERS_DIR / f"{forum_id}_{safe_title}"
    paper_folder.mkdir(exist_ok=True)
    
    print(f"\nğŸ“„ å¤„ç†è®ºæ–‡ / Processing: {title[:60]}...")
    
    # 1. ä¸‹è½½è®ºæ–‡PDF / Download paper PDF
    if hasattr(main_note, 'content') and 'pdf' in main_note.content:
        pdf_url = main_note.content['pdf']
        pdf_path = paper_folder / f"{forum_id}.pdf"
        if not pdf_path.exists():
            print(f"  ğŸ“¥ ä¸‹è½½PDF... / Downloading PDF...")
            download_file(pdf_url, pdf_path)
        else:
            print(f"  âœ“ PDFå·²å­˜åœ¨ / PDF already exists")
    
    # 2. ä¿å­˜è®ºæ–‡å…ƒæ•°æ®ï¼ˆåŒ…å«è¡¨æ ¼æ•°æ®ï¼‰/ Save paper metadata (including table data)
    metadata_path = METADATA_DIR / f"{forum_id}_metadata.json"
    metadata = {
        'forum_id': forum_id,
        'title': title,
        'authors': main_note.content.get('authors', []) if hasattr(main_note, 'content') else [],
        'abstract': main_note.content.get('abstract', '') if hasattr(main_note, 'content') else '',
        'keywords': main_note.content.get('keywords', []) if hasattr(main_note, 'content') else [],
        'openreview_url': f"https://openreview.net/forum?id={forum_id}",
        'created': str(main_note.cdate) if hasattr(main_note, 'cdate') else None,
    }
    
    # æ·»åŠ è¡¨æ ¼æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰/ Add table data (if available)
    # è¡¨æ ¼æ•°æ®æ ¼å¼: {title, status, primary_topic, secondary_topic, human_review, ai_reviewer_1, ai_reviewer_2, ai_reviewer_3, hypothesis_development}
    # Table data format: {title, status, primary_topic, secondary_topic, human_review, ai_reviewer_1, ai_reviewer_2, ai_reviewer_3, hypothesis_development}
    if table_data and forum_id in table_data and table_data[forum_id]:
        # ç›´æ¥å°†è¡¨æ ¼æ•°æ®åˆå¹¶åˆ°å…ƒæ•°æ®ä¸­ï¼ˆè€Œä¸æ˜¯åµŒå¥—åœ¨ table_data å­—æ®µä¸‹ï¼‰
        # Directly merge table data into metadata (instead of nesting under table_data field)
        table_info = table_data[forum_id]
        if isinstance(table_info, dict):
            metadata.update(table_info)
            print(f"  âœ“ åŒ…å«è¡¨æ ¼æ•°æ® / Includes table data")
    
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    # 3. ä¸‹è½½å¹¶ä¿å­˜æ‰€æœ‰reviews / Download and save all reviews
    reviews_data = []
    for i, review in enumerate(paper_info['reviews']):
        review_data = {
            'review_id': review.id,
            'content': review.content if hasattr(review, 'content') else {},
            'rating': review.content.get('rating', '') if hasattr(review, 'content') else '',
            'confidence': review.content.get('confidence', '') if hasattr(review, 'content') else '',
            'created': str(review.cdate) if hasattr(review, 'cdate') else None,
        }
        reviews_data.append(review_data)
    
    reviews_path = REVIEWS_DIR / f"{forum_id}_reviews.json"
    with open(reviews_path, 'w', encoding='utf-8') as f:
        json.dump(reviews_data, f, indent=2, ensure_ascii=False)
    
    if reviews_data:
        print(f"  âœ“ ä¿å­˜äº† {len(reviews_data)} æ¡review / Saved {len(reviews_data)} reviews")
    
    # 4. ä¿å­˜ä»£ç é“¾æ¥ä¿¡æ¯ / Save code link information
    if paper_info['code_attachments']:
        code_info_path = CODE_DIR / f"{forum_id}_code_info.json"
        with open(code_info_path, 'w', encoding='utf-8') as f:
            json.dump(paper_info['code_attachments'], f, indent=2, ensure_ascii=False)
        print(f"  âœ“ æ‰¾åˆ°ä»£ç é“¾æ¥ / Found code links")
    
    # 5. ä¿å­˜å®Œæ•´çš„noteæ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰/ Save complete note data
    complete_data_path = paper_folder / f"{forum_id}_complete.json"
    try:
        # å°†noteå¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        # Convert note objects to serializable dictionaries
        complete_data = {
            'main_note': {
                'id': main_note.id,
                'content': dict(main_note.content) if hasattr(main_note, 'content') else {},
                'invitation': main_note.invitation,
                'cdate': str(main_note.cdate) if hasattr(main_note, 'cdate') else None,
            },
            'reviews': [
                {
                    'id': r.id,
                    'content': dict(r.content) if hasattr(r, 'content') else {},
                    'invitation': r.invitation,
                }
                for r in paper_info['reviews']
            ]
        }
        with open(complete_data_path, 'w', encoding='utf-8') as f:
            json.dump(complete_data, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"  âš ï¸  ä¿å­˜å®Œæ•´æ•°æ®æ—¶å‡ºé”™ / Error saving complete data: {e}")
    
    return True

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰¹é‡ä¸‹è½½æ‰€æœ‰è®ºæ–‡
    Main function: batch download all papers
    """
    print("=" * 60)
    print("ğŸš€ Agents4Science 2025 è®ºæ–‡æ‰¹é‡ä¸‹è½½å·¥å…·")
    print("ğŸš€ Agents4Science 2025 Paper Batch Download Tool")
    print("=" * 60)
    
    # æ–¹æ³•0ï¼šä¼˜å…ˆä» table_data.csv è·å– forum IDsï¼ˆæ¨èæ–¹å¼ï¼‰
    # Method 0: Priority: get forum IDs from table_data.csv (recommended)
    table_data = load_table_data()
    forum_ids = []
    
    if table_data:
        forum_ids = list(table_data.keys())
        print(f"âœ… ä» table_data.csv è¯»å–åˆ° {len(forum_ids)} ä¸ª forum ID / Read {len(forum_ids)} forum IDs from table_data.csv")
    
    # æ–¹æ³•1ï¼šå¦‚æœ table_data.json ä¸å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦å·²æœ‰forum_ids.txtæ–‡ä»¶
    # Method 1: If table_data.json doesn't exist, check if forum_ids.txt exists
    if not forum_ids:
        forum_ids_path = BASE_DIR / "forum_ids.txt"
        if forum_ids_path.exists():
            print(f"ğŸ“„ å‘ç°å·²æœ‰forum_ids.txtæ–‡ä»¶ï¼Œæ­£åœ¨è¯»å–... / Found existing forum_ids.txt, reading...")
            try:
                with open(forum_ids_path, 'r', encoding='utf-8') as f:
                    forum_ids = [line.strip() for line in f if line.strip()]
                if forum_ids:
                    print(f"âœ… ä»æ–‡ä»¶è¯»å–åˆ° {len(forum_ids)} ä¸ªforum ID / Read {len(forum_ids)} forum IDs from file")
            except Exception as e:
                print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ / Failed to read file: {e}")
    
    # æ–¹æ³•2ï¼šå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°è¯•é€šè¿‡APIè·å–
    # Method 2: If file doesn't exist or is empty, try to get via API
    if not forum_ids:
        forum_ids = get_all_papers_from_api()
    
    # æ–¹æ³•3ï¼šå¦‚æœAPIå¤±è´¥ï¼Œä»ç½‘é¡µæå–
    # Method 3: If API fails, extract from webpage
    if not forum_ids:
        submissions_url = "https://agents4science.stanford.edu/submissions.html"
        forum_ids = extract_forum_ids_from_page(submissions_url)
    
    if not forum_ids:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡ / No papers found")
        print("ğŸ’¡ æç¤ºï¼šè¯·è¿è¡Œ extract_table_data.js æå–è¡¨æ ¼æ•°æ®ï¼ˆä¼šç”Ÿæˆ CSV æ ¼å¼ï¼‰ï¼Œæˆ–æ‰‹åŠ¨ç¼–è¾‘ forum_ids.txt æ–‡ä»¶")
        print("ğŸ’¡ Tip: Please run extract_table_data.js to extract table data (will generate CSV format), or manually edit forum_ids.txt file")
        return
    
    # å¦‚æœæ²¡æœ‰è¡¨æ ¼æ•°æ®ï¼Œå°è¯•ä» forum_ids.txt ç”Ÿæˆç©ºçš„è¡¨æ ¼æ•°æ®ç»“æ„
    # If no table data, try to generate empty table data structure from forum_ids.txt
    if not table_data:
        print("âš ï¸  æœªæ‰¾åˆ°è¡¨æ ¼æ•°æ®ï¼Œå°†ä½¿ç”¨ç©ºçš„è¡¨æ ¼æ•°æ®ç»“æ„ / No table data found, will use empty table data structure")
        table_data = {fid: {} for fid in forum_ids}
    
    # ä¿å­˜forum IDåˆ—è¡¨ï¼ˆä½œä¸ºå¤‡ä»½ï¼‰
    # Save forum ID list (as backup)
    forum_ids_path = BASE_DIR / "forum_ids.txt"
    with open(forum_ids_path, 'w', encoding='utf-8') as f:
        for fid in forum_ids:
            f.write(f"{fid}\n")
    print(f"âœ“ å·²ä¿å­˜forum IDåˆ—è¡¨åˆ° / Saved forum ID list to: {forum_ids_path}")
    
    # ä¸‹è½½æ¯ç¯‡è®ºæ–‡
    # Download each paper
    total = len(forum_ids)
    success_count = 0
    failed_count = 0
    
    for idx, forum_id in enumerate(forum_ids, 1):
        print(f"\n[{idx}/{total}] å¤„ç†è®ºæ–‡ID / Processing paper ID: {forum_id}")
        
        try:
            # è·å–è®ºæ–‡è¯¦æƒ…
            # Get paper details
            paper_info = get_paper_details(forum_id)
            
            if paper_info:
                # ä¸‹è½½è®ºæ–‡èµ„æºï¼ˆä¼ å…¥è¡¨æ ¼æ•°æ®ï¼‰
                # Download paper resources (pass table data)
                if download_paper(forum_id, paper_info, table_data):
                    success_count += 1
                else:
                    failed_count += 1
            else:
                failed_count += 1
                print(f"  âŒ æ— æ³•è·å–è®ºæ–‡ä¿¡æ¯ / Unable to get paper information")
        
        except Exception as e:
            failed_count += 1
            print(f"  âŒ å¤„ç†å¤±è´¥ / Processing failed: {e}")
        
        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è¯·æ±‚è¿‡å¿«
        # Add delay to avoid too many requests
        time.sleep(1)
    
    # æ‰“å°æ€»ç»“
    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¸‹è½½å®Œæˆ / Download Complete")
    print("=" * 60)
    print(f"âœ… æˆåŠŸ / Success: {success_count}")
    print(f"âŒ å¤±è´¥ / Failed: {failed_count}")
    print(f"ğŸ“ ä¸‹è½½ç›®å½• / Download directory: {BASE_DIR.absolute()}")
    print("=" * 60)

if __name__ == "__main__":
    main()

