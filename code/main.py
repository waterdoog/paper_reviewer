#!/usr/bin/env python3
"""
Agent4Science Dataset Builder - Downloader
Downloads PDFs, reviews, supplementary materials, and code for papers.
Requires metadata.csv (generated by scrape_metadata.py).
"""

import os
import re
import json
import time
import random
import logging
import subprocess
from pathlib import Path
from urllib.parse import urljoin, urlparse, parse_qs
from typing import Dict, List, Optional, Tuple

import pandas as pd
import requests
from bs4 import BeautifulSoup
from tqdm import tqdm
import openreview

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('downloads/scraper.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# Constants
OPENREVIEW_BASE = "https://openreview.net"
OPENREVIEW_API_BASE = "https://api2.openreview.net"
DOWNLOAD_DIR = Path("downloads")
PDF_DIR = DOWNLOAD_DIR / "pdfs"
REVIEWS_DIR = DOWNLOAD_DIR / "reviews"
SUPPLEMENTARY_DIR = DOWNLOAD_DIR / "supplementary"
CODE_DIR = DOWNLOAD_DIR / "code"

# Request delays (in seconds)
MIN_DELAY = 0.3
MAX_DELAY = 1.0


def setup_directories():
    """Create necessary directories."""
    for dir_path in [DOWNLOAD_DIR, PDF_DIR, REVIEWS_DIR, SUPPLEMENTARY_DIR, CODE_DIR]:
        dir_path.mkdir(parents=True, exist_ok=True)
    logger.info("Directories created/verified")


def polite_delay():
    """Add a random delay between requests."""
    time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))


def load_metadata() -> pd.DataFrame:
    """Load metadata from CSV file."""
    metadata_path = DOWNLOAD_DIR / "metadata.csv"
    
    if not metadata_path.exists():
        logger.error(f"Metadata file not found: {metadata_path}")
        logger.error("Please run 'python code/scrape_metadata.py' first to generate metadata.csv")
        return pd.DataFrame()
    
    try:
        df = pd.read_csv(metadata_path)
        logger.info(f"Loaded {len(df)} papers from {metadata_path}")
        return df
    except Exception as e:
        logger.error(f"Error loading metadata from {metadata_path}: {e}", exc_info=True)
        return pd.DataFrame()


def initialize_openreview_client():
    """Initialize OpenReview API v2 client. Try anonymous first, then authenticated."""
    # First, try anonymous access (no login required for public conferences)
    try:
        client = openreview.api.OpenReviewClient(baseurl=OPENREVIEW_API_BASE)
        # Test if anonymous access works by trying a simple operation
        logger.info("OpenReview client initialized (anonymous access)")
        return client
    except Exception as e:
        logger.debug(f"Anonymous access failed: {e}. Trying authenticated access...")
    
    # If anonymous fails, try with credentials
    username = os.getenv("OPENREVIEW_USERNAME")
    password = os.getenv("OPENREVIEW_PASSWORD")
    
    if not username or not password:
        logger.warning(
            "Anonymous access failed and OPENREVIEW_USERNAME/PASSWORD not set. "
            "Some features may not work. For public conferences, anonymous access should work. "
            "If you see errors, you may need to set credentials."
        )
        # Still return a client - it might work for some operations
        try:
            return openreview.api.OpenReviewClient(baseurl=OPENREVIEW_API_BASE)
        except:
            return None
    
    try:
        client = openreview.api.OpenReviewClient(
            baseurl=OPENREVIEW_API_BASE,
            username=username,
            password=password
        )
        logger.info("OpenReview client initialized (authenticated)")
        return client
    except Exception as e:
        logger.error(f"Error initializing OpenReview client: {e}", exc_info=True)
        return None


def download_pdf(client: Optional[openreview.api.OpenReviewClient], forum_id: str) -> bool:
    """Download PDF for a given forum_id. Try direct download first, then API, then web scraping."""
    pdf_path = PDF_DIR / f"{forum_id}.pdf"
    
    # Always try to download (will overwrite if exists, but check file size first)
    if pdf_path.exists():
        file_size = pdf_path.stat().st_size
        # If file is very small (< 1KB), likely corrupted, re-download
        if file_size > 1024:
            logger.debug(f"PDF already exists: {pdf_path} ({file_size} bytes)")
            return True
        else:
            logger.info(f"PDF exists but seems corrupted ({file_size} bytes), re-downloading: {pdf_path}")
    
    # Method 1: Direct download from OpenReview (simplest and most reliable)
    max_retries = 3
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    for attempt in range(max_retries):
        try:
            direct_pdf_url = f"{OPENREVIEW_BASE}/pdf?id={forum_id}"
            polite_delay()
            # Use stream=True for large PDFs
            response = requests.get(direct_pdf_url, headers=headers, timeout=120, 
                                  allow_redirects=True, stream=True)
            response.raise_for_status()
            
            # Check first chunk to verify it's a PDF
            first_chunk = next(response.iter_content(chunk_size=4), b'')
            if first_chunk[:4] == b'%PDF':
                # Stream download the rest
                with open(pdf_path, 'wb') as f:
                    f.write(first_chunk)
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                
                # Verify file size
                file_size = pdf_path.stat().st_size
                if file_size > 1024:  # At least 1KB
                    logger.info(f"Downloaded PDF via direct link: {pdf_path} ({file_size} bytes)")
                    return True
                else:
                    logger.warning(f"Downloaded PDF too small ({file_size} bytes), retrying...")
                    if pdf_path.exists():
                        pdf_path.unlink()
            else:
                logger.debug(f"Direct PDF link returned non-PDF content for {forum_id}")
                break  # Don't retry if it's not a PDF
                
        except (requests.exceptions.ChunkedEncodingError,
                requests.exceptions.ConnectionError,
                requests.exceptions.IncompleteRead) as e:
            if attempt < max_retries - 1:
                logger.warning(f"PDF download interrupted for {forum_id} (attempt {attempt + 1}/{max_retries}): {e}. Retrying...")
                if pdf_path.exists():
                    pdf_path.unlink()
                time.sleep(2)  # Wait before retry
            else:
                logger.debug(f"Direct download method failed for {forum_id} after {max_retries} attempts: {e}. Trying API...")
        except Exception as e:
            logger.debug(f"Direct download method failed for {forum_id}: {e}. Trying API...")
            break  # Try next method
    
    # Method 2: Try using API to get PDF ID
    if client:
        try:
            polite_delay()
            notes = client.get_all_notes(forum=forum_id, details="all")
            
            # Find submission note (usually the first one or has 'content' with 'pdf')
            pdf_id = None
            for note in notes:
                if hasattr(note, 'content') and isinstance(note.content, dict):
                    if 'pdf' in note.content:
                        pdf_id = note.content['pdf']
                        break
            
            if pdf_id:
                # Download PDF via API
                pdf_url = f"{OPENREVIEW_API_BASE}/pdf/{pdf_id}"
                polite_delay()
                response = requests.get(pdf_url, timeout=60)
                response.raise_for_status()
                
                # Check if response is actually a PDF
                if response.headers.get('content-type', '').startswith('application/pdf') or response.content[:4] == b'%PDF':
                    with open(pdf_path, 'wb') as f:
                        f.write(response.content)
                    logger.info(f"Downloaded PDF via API: {pdf_path}")
                    return True
        except Exception as e:
            logger.debug(f"API method failed for {forum_id}: {e}. Trying web scraping...")
    
    # Method 3: Fallback to web scraping (for public PDFs)
    try:
        forum_url = f"{OPENREVIEW_BASE}/forum?id={forum_id}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        polite_delay()
        response = requests.get(forum_url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find PDF download link
        pdf_link = None
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True).lower()
            if 'pdf' in text or 'download' in text or '/pdf/' in href:
                if href.startswith('http'):
                    pdf_link = href
                else:
                    pdf_link = urljoin(OPENREVIEW_BASE, href)
                break
        
        # Also try to find PDF in iframe or direct links
        if not pdf_link:
            for iframe in soup.find_all('iframe', src=True):
                if 'pdf' in iframe['src'].lower():
                    pdf_link = urljoin(OPENREVIEW_BASE, iframe['src'])
                    break
        
        if pdf_link:
            polite_delay()
            response = requests.get(pdf_link, headers=headers, timeout=60)
            response.raise_for_status()
            
            if response.content[:4] == b'%PDF':
                with open(pdf_path, 'wb') as f:
                    f.write(response.content)
                logger.info(f"Downloaded PDF via web scraping: {pdf_path}")
                return True
    
    except Exception as e:
        logger.debug(f"Web scraping method failed for {forum_id}: {e}")
    
    logger.warning(f"Could not download PDF for forum_id: {forum_id}")
    return False


def download_review_history(client: Optional[openreview.api.OpenReviewClient], forum_id: str) -> bool:
    """Download full review history as JSON."""
    review_path = REVIEWS_DIR / f"{forum_id}.json"
    
    if review_path.exists():
        logger.debug(f"Review history already exists: {review_path}")
        return True
    
    if not client:
        logger.warning(f"Cannot download review history for {forum_id}: client not available")
        return False
    
    try:
        polite_delay()
        notes = client.get_all_notes(forum=forum_id, details="all")
        
        # Convert all notes to JSON
        notes_json = []
        for note in notes:
            try:
                note_json = note.to_json()
                notes_json.append(note_json)
            except Exception as e:
                logger.warning(f"Error converting note to JSON for {forum_id}: {e}")
        
        # Save as JSON
        with open(review_path, 'w', encoding='utf-8') as f:
            json.dump(notes_json, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Saved review history: {review_path} ({len(notes_json)} notes)")
        return True
    
    except Exception as e:
        logger.error(f"Error downloading review history for {forum_id}: {e}", exc_info=True)
        return False


def download_supplementary(forum_id: str, supplementary_link: str = None) -> bool:
    """Download supplementary materials from forum page."""
    supplementary_path = SUPPLEMENTARY_DIR / f"{forum_id}"
    
    # Check if already exists (with any extension)
    existing_files = list(SUPPLEMENTARY_DIR.glob(f"{forum_id}.*"))
    if existing_files:
        logger.debug(f"Supplementary already exists: {existing_files[0]}")
        return True
    
    try:
        forum_url = f"{OPENREVIEW_BASE}/forum?id={forum_id}"
        polite_delay()
        response = requests.get(forum_url, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Find supplementary attachment links
        supp_links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text(strip=True).lower()
            
            # Check if it's a supplementary material link
            if ('supp' in text or 'supplementary' in text or 
                'supp' in href.lower() or 'supplementary' in href.lower()):
                full_url = urljoin(OPENREVIEW_BASE, href) if not href.startswith('http') else href
                supp_links.append((full_url, text))
        
        # Also check if supplementary_link was provided in metadata
        if supplementary_link and supplementary_link not in [url for url, _ in supp_links]:
            supp_links.append((supplementary_link, 'supplementary'))
        
        if not supp_links:
            logger.debug(f"No supplementary materials found for {forum_id}")
            return False
        
        # Download the first supplementary link found
        supp_url, _ = supp_links[0]
        
        # Handle OpenReview attachment URLs
        if '/attachment' in supp_url:
            # Extract attachment ID
            parsed = urlparse(supp_url)
            params = parse_qs(parsed.query)
            attachment_id = params.get('id', [None])[0]
            if attachment_id:
                # Try to get attachment via API or direct download
                attachment_url = f"{OPENREVIEW_BASE}{parsed.path}?{parsed.query}"
                
                # Determine file extension from Content-Disposition or URL
                ext = 'zip'  # default
                if '.' in supp_url:
                    ext = Path(supp_url).suffix or 'zip'
                
                supplementary_path = SUPPLEMENTARY_DIR / f"{forum_id}{ext}"
                
                # Retry mechanism for large file downloads
                max_retries = 3
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                for attempt in range(max_retries):
                    try:
                        polite_delay()
                        # Use stream=True for large files to avoid memory issues
                        response = requests.get(attachment_url, headers=headers, timeout=120, 
                                              allow_redirects=True, stream=True)
                        response.raise_for_status()
                        
                        # Get file extension from Content-Disposition if available
                        content_disposition = response.headers.get('Content-Disposition', '')
                        if 'filename=' in content_disposition:
                            filename = re.search(r'filename[^;=\n]*=(([\'"]).*?\2|[^\s;]+)', content_disposition)
                            if filename:
                                ext = Path(filename.group(1).strip('"\'')).suffix or 'zip'
                                supplementary_path = SUPPLEMENTARY_DIR / f"{forum_id}{ext}"
                        
                        # Stream download to file
                        total_size = 0
                        with open(supplementary_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                if chunk:
                                    f.write(chunk)
                                    total_size += len(chunk)
                        
                        # Verify file was downloaded (at least 100 bytes)
                        if total_size > 100:
                            logger.info(f"Downloaded supplementary: {supplementary_path} ({total_size} bytes)")
                            return True
                        else:
                            logger.warning(f"Downloaded file too small ({total_size} bytes), retrying...")
                            if supplementary_path.exists():
                                supplementary_path.unlink()
                            
                    except (requests.exceptions.ChunkedEncodingError, 
                            requests.exceptions.ConnectionError,
                            requests.exceptions.IncompleteRead) as e:
                        if attempt < max_retries - 1:
                            logger.warning(f"Download interrupted for {forum_id} (attempt {attempt + 1}/{max_retries}): {e}. Retrying...")
                            if supplementary_path.exists():
                                supplementary_path.unlink()
                            time.sleep(2)  # Wait before retry
                        else:
                            logger.error(f"Failed to download supplementary for {forum_id} after {max_retries} attempts: {e}")
                            return False
                    except Exception as e:
                        logger.error(f"Error downloading supplementary for {forum_id}: {e}")
                        if supplementary_path.exists():
                            supplementary_path.unlink()
                        return False
        
        logger.debug(f"Could not download supplementary for {forum_id}")
        return False
    
    except Exception as e:
        logger.error(f"Error downloading supplementary for {forum_id}: {e}", exc_info=True)
        return False


def clone_github_repo(code_link: str, forum_id: str) -> bool:
    """Clone GitHub repository if code_link is a GitHub URL."""
    code_path = CODE_DIR / forum_id
    
    if code_path.exists():
        logger.debug(f"Code directory already exists: {code_path}")
        return True
    
    # Convert NaN/None to empty string and ensure it's a string
    if pd.isna(code_link):
        code_link = ''
    else:
        code_link = str(code_link)
    
    if not code_link or 'github.com' not in code_link.lower():
        return False
    
    try:
        # Extract GitHub repo URL
        # Handle various GitHub URL formats
        if 'github.com' in code_link:
            # Normalize to https://github.com/owner/repo format
            match = re.search(r'github\.com[/:]([^/]+)/([^/\s]+)', code_link)
            if match:
                owner = match.group(1)
                repo = match.group(2).rstrip('.git')
                repo_url = f"https://github.com/{owner}/{repo}.git"
                
                logger.info(f"Cloning GitHub repo: {repo_url}")
                polite_delay()
                
                result = subprocess.run(
                    ['git', 'clone', repo_url, str(code_path)],
                    capture_output=True,
                    text=True,
                    timeout=300
                )
                
                if result.returncode == 0:
                    logger.info(f"Cloned repository: {code_path}")
                    return True
                else:
                    logger.warning(f"Git clone failed for {repo_url}: {result.stderr}")
                    return False
        
        return False
    
    except subprocess.TimeoutExpired:
        logger.error(f"Git clone timeout for {code_link}")
        return False
    except Exception as e:
        logger.error(f"Error cloning GitHub repo {code_link}: {e}", exc_info=True)
        return False


def process_paper(row: pd.Series, client: Optional[openreview.api.OpenReviewClient], stats: Dict[str, int]) -> None:
    """Process a single paper: download PDF, reviews, supplementary, and code."""
    forum_id = row.get('forum_id')
    
    if not forum_id:
        logger.warning(f"Skipping paper with missing forum_id: {row.get('title', 'Unknown')}")
        stats['skipped'] = stats.get('skipped', 0) + 1
        return
    
    logger.info(f"Processing paper: {forum_id} - {row.get('title', 'Unknown')[:50]}...")
    
    # Download PDF (works with or without client - has fallback to web scraping)
    if download_pdf(client, forum_id):
        stats['pdfs'] = stats.get('pdfs', 0) + 1
    else:
        stats['pdf_failures'] = stats.get('pdf_failures', 0) + 1
    
    # Download review history (requires client, but can be anonymous)
    if client:
        if download_review_history(client, forum_id):
            stats['reviews'] = stats.get('reviews', 0) + 1
        else:
            stats['review_failures'] = stats.get('review_failures', 0) + 1
    else:
        logger.warning(f"OpenReview client not available, skipping review history for {forum_id}")
        stats['review_failures'] = stats.get('review_failures', 0) + 1
    
    # Download supplementary materials
    supplementary_link = row.get('supplementary_link', '')
    # Convert NaN/None to empty string and ensure it's a string
    if pd.isna(supplementary_link):
        supplementary_link = ''
    else:
        supplementary_link = str(supplementary_link)
    
    if download_supplementary(forum_id, supplementary_link):
        stats['supplementary'] = stats.get('supplementary', 0) + 1
    else:
        stats['supplementary_failures'] = stats.get('supplementary_failures', 0) + 1
    
    # Clone GitHub repo if applicable
    code_link = row.get('code_link', '')
    # Convert NaN/None to empty string and ensure it's a string
    if pd.isna(code_link):
        code_link = ''
    else:
        code_link = str(code_link)
    
    if code_link and 'github.com' in code_link.lower():
        # Check if code_link is the same as supplementary (don't download twice)
        if supplementary_link and code_link in supplementary_link:
            logger.debug(f"Code link is same as supplementary, skipping: {forum_id}")
        else:
            if clone_github_repo(code_link, forum_id):
                stats['github_repos'] = stats.get('github_repos', 0) + 1
            else:
                stats['github_failures'] = stats.get('github_failures', 0) + 1
    elif code_link and '/attachment' in code_link:
        # OpenReview ZIP attachment - already handled by supplementary
        logger.debug(f"Code link is OpenReview attachment, handled by supplementary: {forum_id}")


def main():
    """Main function to build the dataset."""
    logger.info("=" * 60)
    logger.info("Agent4Science Dataset Builder - Starting")
    logger.info("=" * 60)
    
    # Setup directories
    setup_directories()
    
    # Load metadata from CSV
    df = load_metadata()
    
    if df.empty:
        logger.error("No metadata found. Please run 'python code/scrape_metadata.py' first.")
        return
    
    # Log missing fields
    missing_fields = []
    for col in ['forum_id', 'title', 'status']:
        if col in df.columns:
            missing_count = df[col].isna().sum() + (df[col] == '').sum()
            if missing_count > 0:
                missing_fields.append(f"{col}: {missing_count} missing")
    
    if missing_fields:
        logger.warning(f"Missing fields: {', '.join(missing_fields)}")
    
    # Initialize OpenReview client
    client = initialize_openreview_client()
    
    # Process each paper
    stats = {
        'total': len(df),
        'pdfs': 0,
        'reviews': 0,
        'supplementary': 0,
        'github_repos': 0,
        'skipped': 0,
        'pdf_failures': 0,
        'review_failures': 0,
        'supplementary_failures': 0,
        'github_failures': 0
    }
    
    logger.info(f"Processing {len(df)} papers...")
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="Processing papers"):
        try:
            process_paper(row, client, stats)
        except Exception as e:
            logger.error(f"Error processing paper {row.get('forum_id', 'unknown')}: {e}", exc_info=True)
            stats['skipped'] = stats.get('skipped', 0) + 1
    
    # Print summary
    logger.info("=" * 60)
    logger.info("Dataset Build Summary")
    logger.info("=" * 60)
    logger.info(f"Total papers processed: {stats['total']}")
    logger.info(f"PDFs downloaded: {stats['pdfs']}")
    logger.info(f"Review histories saved: {stats['reviews']}")
    logger.info(f"Supplementary files saved: {stats['supplementary']}")
    logger.info(f"GitHub repos cloned: {stats['github_repos']}")
    logger.info(f"Skipped papers: {stats['skipped']}")
    if stats.get('pdf_failures', 0) > 0:
        logger.warning(f"PDF download failures: {stats['pdf_failures']}")
    if stats.get('review_failures', 0) > 0:
        logger.warning(f"Review download failures: {stats['review_failures']}")
    if stats.get('supplementary_failures', 0) > 0:
        logger.warning(f"Supplementary download failures: {stats['supplementary_failures']}")
    if stats.get('github_failures', 0) > 0:
        logger.warning(f"GitHub clone failures: {stats['github_failures']}")
    logger.info("=" * 60)
    logger.info("Dataset build complete!")


if __name__ == "__main__":
    main()
