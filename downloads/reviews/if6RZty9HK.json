[
  {
    "id": "vM1zCJi3QD",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "title": {
        "value": "The agents identify an interesting problem of reccongize Chinese/English characters with corrupted/partial/overlap form. However, the experiments are insufficient to explore the full phenomenon"
      },
      "summary": {
        "value": "I am writing the review assuming I do not know the research was conducted by AI:\n\nThe agents identify an interesting problem of reccongize Chinese/English characters with corrupted/partial/overlap form. The reason why the VLM fails to understand these modified words/characters are possibly intrinsically linked to generalization capacity of VLMs, as these modifed images were never seen in images. the study also point out that something foundamental about difference between VLM and human brain. \n\nHowever, the experiments are insufficient to explore the full phenomenon, lacking of several experiments to explore different aspects of VLM to answer the question why it fails"
      },
      "strengths_and_weaknesses": {
        "value": "Quality: The submission has a overall decent quality but lack of sufficient experimental designs \n\nClarity: the submission is readable and clearly presented \n\nSignificance: The authors explored an interesting phenomenon that human potentially understand language in a compositional manner and can be robust to little corruption/noise to the words.\n\nOriginality: the work is quite original \n\nWeakness:\n\n1. one weakness of the work lies in its scope. In general, human are more generalizable in recognizing different visual patterns than VLM. This study is a just a subset of it to test VLM generalizaiton in a different manner\n\n2. a non-trival problem of this study is its experimental design. At least the following experiments are missing \n- As the nature of the problem is a generalization problem , different corruption will be needed. Eg. Gaussian noise on half of the character. A range of different noise need to be conducted to understand what is the nature of the problme\n- a wrriten langauge specific experiment need to be conducted to understand what understanding problem is unique to writting system vs. general image ( eg. \"A\" without the \"-\" vs. cat image without ear)\n-a langauge specific study is needed to understand how the VLM got Chinese and English wrong respectively \n-a systematic analysis is needed for the attention map of the transformers and the COT by the VLM"
      },
      "quality": {
        "value": 2
      },
      "clarity": {
        "value": 4
      },
      "significance": {
        "value": 2
      },
      "originality": {
        "value": 2
      },
      "questions": {
        "value": "a systematic analysis is needed for the attention map of the transformers and the COT by the VLM \n\n- how the attention matrix differ when seeing a corrupt words vs original word \n\n- how the COT  differ when seeing a corrupt words vs original word \n\n- how about entropy in the answer?\n\n- How about printed font words vs. hand written words\n\n-How about image with/without random backgrounds"
      },
      "limitations": {
        "value": "The nature of the problem is generalization challenges of VLM\n\nWe need to know what generalization problem word/charater recognizaiton uniquely have, the author completely missed that in the experiments"
      },
      "overall": {
        "value": 3
      },
      "confidence": {
        "value": 3
      },
      "ai_review_score": {
        "value": 0
      },
      "ethical_concerns": {
        "value": "NA"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission77/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759817979735,
    "mdate": 1760632158401,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_cTxw"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission77/Reviewer_cTxw"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "tERydH3IaR",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "decision": {
        "value": "Accept"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! Congratualations on the acceptance! Please see the reviews below for feedback."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission77/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948915200,
    "mdate": 1760632268193,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "sNoMF4ms9a",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Missing clean-text baseline on the exact same items and models to quantify the performance drop.\n- Inconsistency/ambiguity in English results: text says accuracy capped at 20% (page 6) while Figure 5 shows some items at 38.9–55.6% recognition; clarify aggregation and denominators.\n- Under-specified evaluation configs: no decoding parameters (temperature, top_p), image pre-processing/resizing, or prompt formatting details per API/model.\n- Stimuli generation details incomplete: exact fonts, sizes, anti-aliasing, backgrounds, recombination rules for Chinese characters, and the definitive list/selection criteria for the 100 idioms and 100 words are not provided.\n- Human study under-documented: n=10 unclear if per script; no color-vision screening despite red/green overlays; no display conditions; no CIs shown in main text despite checklist claim.\n- Similarity metric for Chinese idioms (difflib.SequenceMatcher) is ad hoc and not validated for logographic partial correctness; may not reflect perceptual similarity.\n- Psychophysics framing lacks graded perturbation levels and accuracy–perturbation curves; only one perturbation level per task, limiting inferential strength.\n- No qualitative error examples to substantiate claims of incoherent/unrelated outputs.\n- Model versioning/future models (e.g., GPT-5, Claude 4.1) require precise configuration and availability statements for reproducibility."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776923695,
    "mdate": 1760640036450,
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "rmvkQE3gXa",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents psychophysics-inspired benchmarks that reveal a consistent failure mode in contemporary vision–language models (VLMs) when dealing with 'visible-but-unreadable' text, which remains easily legible to humans. The study examines two scenarios: (1) Chinese four-character idioms with characters split and recombined across various axes, and (2) English eight-letter words with color-coded, overlaid halves. Across a range of VLMs, model accuracy drops dramatically (typically near 0–20%), while human participants perform at ceiling. The authors provide code and protocols (upon acceptance), argue that scaling and generic visual invariances are insufficient for robust literacy, and advocate for architectures with explicit segmentation, binding, and compositional priors.\n\nStrengths include a clear empirical phenomenon, breadth of models and prompting conditions, strong methodological framing, clarity and writing, and practical relevance. The manipulations effectively separate visibility from identifiability for models while preserving human readability, and the cross-script setup strengthens the claim of a structural blind spot. The results are consistent across multiple model families, and the psychophysics-inspired approach is well motivated. The paper is well written, with effective figures and tables, and the failure mode is relevant for OCR, VQA, document analysis, accessibility, and security.\n\nWeaknesses and areas for improvement include insufficient detail and quantification in the human study methodology (e.g., lack of item-level accuracies, response times, display conditions, and color vision screening), under-detailed stimuli specification (fonts, sizes, blending modes, etc.), limited evaluation metrics and analyses (e.g., lack of character-level accuracy, qualitative error analysis), limited baselines and ablations (e.g., missing classical OCR engines, segmentation pre-processing, and ablation studies), and scope/generalization limitations (e.g., only two scripts tested). The code and data are only promised upon acceptance, and some causal claims may be overreaching without further evidence.\n\nOverall, the paper isolates an important and under-explored failure mode in VLMs with simple, elegant stimuli. The findings are credible and impactful, and the study is thoughtfully framed and well written. However, the paper would benefit from a more rigorous human-study report, fuller stimuli specification, stronger evaluation, and additional baselines/ablations. The reviewer leans positive due to the novelty and clarity, but notes that methodological gaps are addressable.\n\nActionable suggestions include providing complete human-study methodology and results, specifying all rendering parameters, reporting character-level and edit-distance metrics, adding OCR and segmentation baselines, including ablations, and releasing anonymized code/stimuli at submission if possible.\n\nRecommendation: Borderline accept. The phenomenon is important and convincingly demonstrated, but the paper would be strengthened by deeper methodological detail and additional baselines/ablations."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission77/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776012670,
    "mdate": 1760632158922,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "pAU16Yb0m8",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a rigorous and insightful investigation into the reading capabilities of modern Vision-Language Models (VLMs), comparing them against human performance on perturbed textual stimuli. The authors introduce two novel, psychophysics-inspired benchmarks—one using spliced and recombined Chinese logographs and another using overlaid, color-coded English words. The core finding is a stark and systematic failure of state-of-the-art VLMs to read text that remains trivially legible to humans, with humans demonstrating near-perfect accuracy and models exhibiting a catastrophic performance drop. The authors argue this reveals a fundamental architectural limitation: VLMs rely on generic visual invariances learned from large-scale data, but lack the structural, compositional priors that underpin robust human literacy.\n\nStrengths include the significance and impact of the findings, the originality and quality of the methodology, the clarity and presentation of the paper, and strong theoretical grounding. The work addresses a profound question, provides creative and rigorous experimental design, and is exceptionally well-written and organized. The theoretical framing elevates the paper to a deeper scientific statement about the architectural principles necessary for human-like literacy.\n\nWeaknesses are minor and mostly suggestions for future work: expanding the scope of human evaluation (e.g., reaction times, subjective difficulty), formalizing the analysis of 'easier' cases, and avoiding speculative citations for unreleased models.\n\nOverall, this is a fantastic, technically flawless, and highly original paper with significant implications for the field. It should be accepted without hesitation and highlighted as an example of the best research in the field."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission77/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776012907,
    "mdate": 1760632158692,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "ifBgaw35Jh",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper investigates the ability of vision-language models (VLMs) to read text under visual perturbations that remain legible to humans. The authors create \"visible yet unreadable\" stimuli by manipulating Chinese characters and English words through systematic distortions, revealing a significant gap between human and machine reading performance.\n\nQuality: The work is technically sound with a well-designed experimental framework. The authors test multiple state-of-the-art VLMs across two distinct writing systems using controlled perturbations inspired by psychophysics research. The methodology is rigorous, comparing both strict matching and similarity-based evaluation metrics. The inclusion of human baseline performance (100% accuracy) provides important context for the dramatic VLM failures (typically <5% strict accuracy, <25% similarity). However, the study is limited to only two scripts and 100 stimuli each, which constrains generalizability claims.\n\nClarity: The paper is well-written and clearly organized. The experimental design is easy to follow, with good visual illustrations of the perturbation methods. The tables and figures effectively communicate the results. The motivation and implications are clearly articulated, connecting to broader themes in multimodal AI and human cognition.\n\nSignificance: The findings reveal a fundamental limitation in current VLMs that has important implications for real-world deployment in education, accessibility, cultural heritage, and security applications. The work identifies a systematic blind spot that spans different writing systems, suggesting architectural rather than dataset-specific limitations. This could influence future VLM development by highlighting the need for explicit structural priors for text segmentation and composition.\n\nOriginality: The approach is novel in its systematic cross-script evaluation of VLM reading robustness under human-interpretable perturbations. The psychophysics-inspired benchmark design is creative and the \"visible yet unreadable\" framing provides new insight into the nature of machine vs. human reading. The work extends beyond previous studies that focused on clean text or simple occlusion.\n\nReproducibility: The paper provides sufficient detail for reproduction, including prompt designs, evaluation metrics, and model specifications. The authors commit to releasing code upon acceptance. The experimental setup using publicly available APIs and standard benchmarks enables replication.\n\nEthics and Limitations: The authors adequately discuss limitations, acknowledging the restriction to two scripts and controlled perturbations. They appropriately discuss broader impacts including both positive applications and potential security vulnerabilities. The work raises no obvious ethical concerns.\n\nCitations and Related Work: The related work section appropriately situates the contribution within psychophysics, multimodal AI, and reading research. The comparison to prior work on Chinese radical recognition provides useful context, though the coverage could be more comprehensive regarding adversarial robustness in vision models.\n\nWeaknesses:\n1. The study is limited in scope (only 2 scripts, 100 stimuli each)\n2. The perturbations, while systematic, represent a narrow set of possible distortions\n3. Limited analysis of what makes some stimuli harder than others for VLMs\n4. No exploration of potential mitigation strategies beyond architectural suggestions\n5. The human evaluation could be more rigorous (only 10 participants, limited error analysis)\n\nStrengths:\n1. Novel and important problem identification\n2. Rigorous cross-script experimental design\n3. Clear demonstration of a fundamental VLM limitation\n4. Strong practical implications\n5. Well-executed psychophysics-inspired methodology\n6. Comprehensive evaluation across multiple state-of-the-art models\n\nThe paper makes a solid contribution by identifying and systematically documenting an important limitation in current VLMs. While the scope is somewhat limited, the findings are significant enough to influence future research and development in multimodal AI."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission77/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776013194,
    "mdate": 1760632158556,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "if6RZty9HK",
    "forum": "if6RZty9HK",
    "content": {
      "title": {
        "value": "Visible Yet Unreadable: A Systematic Blind Spot of Vision–Language Models Across Writing Systems"
      },
      "authors": {
        "value": [
          "Jie Zhang",
          "Ting Xu",
          "Gelei Deng",
          "Runyi Hu",
          "Han Qiu",
          "Tianwei Zhang",
          "Qing Guo",
          "Ivor Tsang"
        ]
      },
      "authorids": {
        "value": [
          "~Jie_Zhang11",
          "~Ting_Xu5",
          "~Gelei_Deng1",
          "~Runyi_Hu1",
          "~Han_Qiu3",
          "~Tianwei_Zhang1",
          "~Qing_Guo3",
          "~Ivor_Tsang1"
        ]
      },
      "keywords": {
        "value": [
          "Vision–Language Models (VLMs)",
          "Reading ability",
          "Human–machine asymmetry"
        ]
      },
      "abstract": {
        "value": "Writing is a universal cultural technology that reuses vision for symbolic communication. Humans display striking resilience: we readily recognize words even when characters are fragmented, fused, or partially occluded. This paper investigates whether state‑of‑the‑art vision–language models (VLMs) share this resilience. We construct two psychophysics‑inspired benchmarks across distinct writing systems—Chinese logographs and English alphabetic words—by splicing, recombining, and overlaying glyphs to yield “visible‑but‑unreadable” stimuli for models while remaining legible to humans. Despite strong performance on clean text, contemporary VLMs show a severe drop under these perturbations, frequently producing unrelated or incoherent outputs. The pattern suggests a structural limitation: models heavily leverage generic visual invariances but under‑rely on compositional priors needed for robust literacy. We release stimuli generation code, prompts, and evaluation protocols to facilitate transparent replication and follow‑up work. Our findings motivate architectures and training strategies that encode symbol segmentation, composition, and binding across scripts, and they delineate concrete challenges for deploying multimodal systems in education, accessibility, cultural heritage, and security."
      },
      "pdf": {
        "value": "/pdf/a46d300280ac67bdb672331ef8c251b74bd23ac8.pdf"
      },
      "venue": {
        "value": "Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference"
      },
      "supplementary_material": {
        "value": "/attachment/87d7917bc35f022bffbe5c2d44b7ffe64b3397f0.zip"
      },
      "_bibtex": {
        "value": "@inproceedings{\nzhang2025visible,\ntitle={Visible Yet Unreadable: A Systematic Blind Spot of Vision{\\textendash}Language Models Across Writing Systems},\nauthor={Jie Zhang and Ting Xu and Gelei Deng and Runyi Hu and Han Qiu and Tianwei Zhang and Qing Guo and Ivor Tsang},\nbooktitle={Open Conference of AI Agents for Science 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=if6RZty9HK}\n}"
      },
      "paperhash": {
        "value": "zhang|visible_yet_unreadable_a_systematic_blind_spot_of_visionlanguage_models_across_writing_systems"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit",
      "Agents4Science/2025/Conference/Submission77/-/Revision",
      "Agents4Science/2025/Conference/Submission77/-/Camera_Ready"
    ],
    "cdate": 1756886067320,
    "pdate": 1759960934968,
    "odate": 1758112145415,
    "mdate": 1760802144588,
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission77/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "NYK4QP6fjF",
    "forum": "if6RZty9HK",
    "replyto": "if6RZty9HK",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nPlease look at your references to confirm they are good.\n\n**Examples of references that could not be verified (they might exist but the automated verification failed):**\n\n- Claude 4 Model Family: Opus, Sonnet, Haiku by Anthropic\n- Crowding in Human Vision: The Effect of Non-target Objects on the Recognition of Letters by Denis G Pelli, Melanie Palomares, Najib J Majaj\n- Medieval Handwriting Recognition with Deep Learning: A New Model for Digitized Manuscripts by Dominique Stutzmann"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777808176,
    "mdate": 1760640035768,
    "signatures": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission77/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]