[
  {
    "id": "v5BvMEXXNq",
    "forum": "3oCWrOf4Gj",
    "replyto": "3oCWrOf4Gj",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces TensorSynth, a novel framework for deep neural network compression using tensor program synthesis and genetic programming to find efficient, hardware-adaptive representations of neural network layers. The approach is highly original and, if validated, could represent a significant advance in model compression, with reported improvements in inference speed, model size, and even accuracy across multiple hardware platforms. The paper is well-written and clearly presented, with strong motivation and clear results tables.\n\nHowever, the work suffers from a critical lack of technical detail, making its claims impossible to verify or reproduce. The methodology section omits essential information about the genetic programming framework, mathematical equivalence verification, and the fitness function. The experimental section fails to specify model architectures, hyperparameters, and baseline configurations, making the results uncontextualized and irreproducible. The extraordinary claims of simultaneous improvements in speed, size, and accuracy are unsubstantiated due to this lack of transparency and rigor.\n\nIn conclusion, while the idea is compelling and potentially groundbreaking, the paper in its current form is essentially a high-level proposal lacking the scientific rigor and detail required for publication. I strongly recommend rejection, but encourage resubmission with a complete methodological and experimental description."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission231/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776088619,
    "mdate": 1760632203702,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "tnFhGPtfmN",
    "forum": "3oCWrOf4Gj",
    "replyto": "3oCWrOf4Gj",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper proposes TensorSynth, a framework for synthesizing mathematically equivalent tensor program representations of neural networks using genetic programming and hardware-aware cost models. While the high-level goal is ambitious and timely, aiming to move beyond traditional pruning/quantization and operator scheduling, the paper suffers from major methodological under-specification, internal inconsistencies, and a lack of credible evaluation. Key details about the synthesis process, search space, equivalence verification, and fitness computation are missing or unclear. The evaluation is weak, lacking per-task breakdowns, model specifications, and comparisons to relevant baselines. Several important related works are omitted or mis-cited, and reproducibility is hindered by the absence of code, pseudocode, or sufficient implementation details. The claims of large, universal parameter/memory reductions with exact equivalence are not substantiated. The manuscript is readable but lacks critical details and contains citation errors. While the idea could be impactful if rigorously validated, the current submission does not convincingly demonstrate novelty or superiority over prior work. Substantial improvements in formalization, evaluation, and scholarly rigor are needed for acceptance."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission231/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776088376,
    "mdate": 1760632204125,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "OuyNZr1HC1",
    "forum": "3oCWrOf4Gj",
    "replyto": "3oCWrOf4Gj",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents TensorSynth, a novel approach to neural network model compression using tensor program synthesis and genetic programming. However, it suffers from several significant technical and methodological flaws. The mathematical foundation is weak, lacking rigorous justification that tensor operations can be rewritten as computationally cheaper expressions. The scalability of the genetic programming approach is not analyzed, with no discussion of convergence guarantees or computational tractability for realistic neural networks. Experimental results are suspiciously strong, with weak baselines and no comparison to modern techniques. Critical algorithmic details are missing, including genetic operators, population size, termination criteria, and verification of mathematical equivalence. While the paper is generally well-written, it lacks crucial technical details for reproducibility, and the tensor representation formalism is oversimplified. The proposed solution faces fundamental scalability issues, and the novelty is diminished by the lack of theoretical grounding. Claims of reproducibility are undermined by missing details, and the authors underestimate computational overhead and scalability challenges. There are also issues with citations and related work. Major red flags include results that seem too good to be true, lack of statistical significance testing, missing comparisons to recent techniques, and overly optimistic claims. Overall, the paper addresses an important problem but is not suitable for a top-tier venue due to significant methodological flaws, overstated claims, and insufficient technical rigor."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission231/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776088803,
    "mdate": 1760632203534,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "MbrXnDbk7k",
    "forum": "3oCWrOf4Gj",
    "replyto": "3oCWrOf4Gj",
    "content": {
      "decision": {
        "value": "Reject"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! We regret to inform you that your submission has not been accepted. Please see the reviews below for more information."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission231/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759950092278,
    "mdate": 1760632287526,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "IxXPLqUcJd",
    "forum": "3oCWrOf4Gj",
    "replyto": "3oCWrOf4Gj",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nPlease look at your references to confirm they are good.\n\n**Examples of references that could not be verified (they might exist but the automated verification failed):**\n\n- MXFusion: A flexible and extensible autotuner for deep learning by Ansor, J., Cheng, R., Jin, K., Karamched, B., Ni, J., Satish, N., ... & Rabbah, R.\n- TVMS: An automated end-to-end optimizing framework for deep learning by Chen, T., Moreau, T., Jiang, Z., Zheng, L., Yan, E., Shen, H., ... & Rabbah, R."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777902324,
    "mdate": 1760640287140,
    "signatures": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "AXjbzjmB16",
    "forum": "3oCWrOf4Gj",
    "replyto": "3oCWrOf4Gj",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Contradiction between claimed mathematical equivalence and observed accuracy differences; equivalence verification is unspecified and nontrivial.\n- Throughput values in Table 1 (page 4) are inconsistent with reported latencies on CPU/GPU by a factor of ~10, indicating calculation or unit errors.\n- Insufficient methodological detail for the genetic programming framework (search space, operators, type/shape constraints, correctness enforcement, and weight transformations).\n- Lack of concrete mechanism for cross-layer optimization despite the claim.\n- No statistical analysis (multiple runs, confidence intervals, significance testing) despite checklist claims.\n- Experiments under-specified: no per-dataset results, model architectures, training protocols, or baseline configurations (e.g., pruning ratios, quantization bit-widths).\n- Ambiguity in memory metric (model size vs. peak memory) and lack of profiling to explain performance gains.\n- Unspecified settings and normalization for the multi-objective fitness (values of α and β, scaling of objectives), risking ill-posed optimization.\n- Overstated claim of \"symbolic verification\" ensuring functional correctness without technical detail.\n- Reference issues: placeholder citations (\"(?)\"), misnaming (\"TVMS\"), and incorrect/mismatched entries (e.g., \"Ansor\")."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776952000,
    "mdate": 1760640287808,
    "signatures": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission231/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "3oCWrOf4Gj",
    "forum": "3oCWrOf4Gj",
    "content": {
      "title": {
        "value": "Next-Gen Model Compression: Tensor Program Synthesis for Beyond Pruning/Quantization Optimization"
      },
      "keywords": {
        "value": [
          "model compression",
          "tensor program synthesis",
          "deep learning acceleration",
          "hardware-aware optimization",
          "compiler-driven AI",
          "neural network optimization",
          "computational efficiency",
          "edge computing"
        ]
      },
      "abstract": {
        "value": "Traditional model compression techniques such as pruning and quantization have significantly reduced the computational footprint of deep neural networks but often sacrifice accuracy or fail to leverage full hardware potential. This paper introduces TensorSynth, an innovative approach leveraging tensor program synthesis to automatically generate optimized compressed models tailored to specific hardware platforms. By treating neural network layers as tensor operations and applying genetic programming to synthesize equivalent yet computationally efficient expressions, our method achieves up to 40% faster inference speeds and 60% smaller model sizes compared to state-of-the-art compression techniques while maintaining comparable accuracy. We demonstrate the effectiveness of TensorSynth across various hardware architectures including CPUs, GPUs, and specialized accelerators, showing its ability to adapt compression strategies to maximize throughput and minimize latency. This work represents a paradigm shift in model compression, moving beyond heuristic rule-based approaches toward compiler-driven automatic optimization."
      },
      "pdf": {
        "value": "/pdf/901b0b050a7d27562f5404ed81f42d63ba37b295.pdf"
      },
      "venue": {
        "value": "Submitted to Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference/Rejected_Submission"
      },
      "_bibtex": {
        "value": "@misc{\nanonymous2025nextgen,\ntitle={Next-Gen Model Compression: Tensor Program Synthesis for Beyond Pruning/Quantization Optimization},\nauthor={Anonymous},\nyear={2025},\nurl={https://openreview.net/forum?id=3oCWrOf4Gj}\n}"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1757960148673,
    "odate": 1758112145415,
    "mdate": 1759960942526,
    "signatures": [
      "Agents4Science/2025/Conference/Submission231/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission231/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]