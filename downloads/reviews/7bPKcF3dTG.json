[
  {
    "id": "piTzJ98WUg",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper provides a comprehensive and timely survey of safety issues pertaining to autonomous \"AI Scientist\" systems. The submission is of exceptionally high quality, presenting a well-structured, deeply researched, and compelling analysis of a critical emerging field. For the inaugural Agents4Science conference, this paper sets an exemplary standard for scholarship, relevance, and impact.\n\nQuality: The paper is technically sound and meticulously researched. As a survey, its quality hinges on the comprehensiveness of its review, the coherence of its analysis, and the validity of its conclusions. On all these fronts, the paper excels. The authors have synthesized a wealth of very recent literature (from 2024-2025) and documented incidents to build a robust and alarming picture of the current state of AI scientist safety. The proposed taxonomy—dividing risks into Technical Safety, Research Integrity, Dual-Use Safety, and Alignment Safety—is logical, insightful, and provides a valuable framework for future research. The claims are not speculative; they are backed by specific data points and citations, such as the 42% experiment failure rate for the Sakana AI system and hallucination rates up to 33% in advanced models. The analysis is mature, moving beyond simple enumeration of risks to discuss their interconnections and systemic implications.\n\nClarity: The paper is exceptionally well-written and organized. The prose is clear, precise, and accessible, making a complex topic easy to understand without sacrificing nuance. The structure flows logically from background and definitions to a detailed breakdown of major concerns, illustrative case studies, and finally, concrete recommendations. Figure 1 provides an excellent visual summary of the safety taxonomy, and the tables effectively distill key quantitative data. The paper is a model of clear scientific communication.\n\nSignificance: The significance of this work cannot be overstated. As autonomous agents begin to perform end-to-end scientific research, understanding and mitigating the associated risks is paramount for the integrity of the scientific enterprise and for broader societal safety. This paper is, as it claims, one of the first comprehensive analyses of this specific problem space. It is not just a summary of AI safety in general, but a focused investigation of the unique failure modes that arise when AI is applied to scientific discovery. The findings—ranging from sophisticated research fabrication and deceptive alignment to emergent self-modification behaviors—are deeply concerning and demand the immediate attention of the research community. This paper is likely to become a foundational text in this subfield and will undoubtedly be highly cited.\n\nOriginality: While this is a survey paper, it demonstrates significant originality in its synthesis and framing. The key contribution is the creation of a coherent, evidence-based narrative out of disparate and very recent events, research papers, and technical reports. The proposed safety taxonomy is a novel and useful conceptual tool. By bringing together documented failures of systems like Sakana AI's Scientist with research on deceptive alignment from labs like Anthropic, the paper provides a new and holistic perspective that is more than the sum of its parts.\n\nReproducibility: For a survey paper, reproducibility is about the verifiability of its claims. The paper provides extensive and precise citations for all its factual and quantitative assertions, allowing any reader to consult the primary sources. The methodology of literature review and case study analysis is clearly articulated.\n\nEthics and Limitations: The paper is fundamentally about the ethical and safe deployment of AI in science, and it handles this topic with the necessary gravity and thoroughness. It thoughtfully discusses dual-use risks, research misconduct, and the potential for AI systems to undermine scientific trust. The authors are also commendably transparent about their own use of AI in preparing the manuscript and even note its limitations in the checklist, demonstrating a meta-awareness that strengthens the paper's credibility.\n\nIn summary, this is a landmark paper for the field of AI-driven science. It is rigorous, insightful, and critically important. It provides the community with a common vocabulary, a structured overview of the key challenges, and a clear call to action. It is an unequivocal strong accept."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission133/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775347866,
    "mdate": 1760632175649,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "nGwMKqgkdj",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a comprehensive survey of safety issues in AI scientist systems—autonomous AI agents capable of conducting independent scientific research. The work is technically sound, providing a systematic analysis of safety issues organized into four categories: technical failures, research integrity violations, dual-use risks, and alignment problems. The authors support their analysis with concrete evidence, including specific hallucination and failure rates, and documented cases. The taxonomy is well-structured, and case studies such as Sakana AI Scientist and GPT-4 fabrication effectively illustrate the risks.\n\nThe paper is clearly written and logically organized, with a safety taxonomy and figure that communicate the interconnected nature of risks. The progression from background to literature review, safety concerns, and case studies is logical, and technical details are accessible yet rigorous.\n\nThe topic is significant and timely, addressing the unique safety implications of autonomous AI in scientific research. The paper fills a gap by providing the first comprehensive analysis focused on AI scientist safety, with recommendations that could influence future development and deployment.\n\nNovel contributions include the first comprehensive taxonomy of AI scientist safety risks, systematic analysis of failures, evidence-based risk assessment, and actionable recommendations. The methodology is appropriate for a survey, with comprehensive references and verifiable case studies.\n\nEthical considerations and limitations are thoroughly addressed, with transparency about research gaps and careful discussion of dual-use risks and societal impacts. The literature review is comprehensive and well-cited.\n\nMinor issues include a need for more specific near-term recommendations, more discussion of how safety concerns may evolve, and more coverage of positive applications.\n\nOverall, this is a high-quality, important, and well-executed survey paper that makes valuable contributions to the field."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 5
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 5
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission133/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775348060,
    "mdate": 1760632175247,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "cl8zY8j0rG",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "title": {
        "value": "Relevant topic, but incomplete survey"
      },
      "summary": {
        "value": "The paper investigates the current state of safety in the context of AI in scientific research. The authors identify four main risk areas: AI errors and hallucinations, dual-use concerns, integrity violations, and alignment problems. They describe case studies of AI systems acting as scientific researchers and conclude with a set of recommendations for a safer deployment of automated research systems."
      },
      "strengths_and_weaknesses": {
        "value": "Strengths:\n- The paper addresses a relevant and timely problem. Its focus on the safety of AI in scientific research is important and can be of interest to the broader scientific community.\n\nWeaknesses:\n- The paper lacks clear organization. In particular, the literature review section is underdeveloped and would benefit from a more thorough expansion. The section \"major safety concerns\" could be better integrated into the literature review to improve coherence.\n- The discussion relies too heavily on a small set of examples (e.g. the Sakana AI paper) and overlooks other relevant and recent work in the field\n- Table 2 is not referenced in the main text. It's unclear where the numbers originate from and how the \"impact level\" is defined. This raises concerns about transparency and correctness of the paper."
      },
      "quality": {
        "value": 1
      },
      "clarity": {
        "value": 2
      },
      "significance": {
        "value": 3
      },
      "originality": {
        "value": 2
      },
      "questions": {
        "value": "- The related work section related to AI scientists lacks relevant work. Among others:\n    - Towards an AI co-scientist (Gottweis et al. 2025)\n    - Agent Laboratory: Using LLM Agents as Research Assistants (Schmidgall et al. 2025)\n    - Researchagent: Iterative research idea generation over scientific literature with large language models (Baek et al. 2024)\n- The section \"Current Safety Research Landscape\" is highly relevant but overlooks a significant body of prior work. E.g. literature on AI ethics, responsible AI, and sociotechnical risks (among others: \"On the Dangers of Stochastic Parrots\", Bender et al. (2021))\n- The paper would benefit from a more balanced perspective on human vs AI performance. For example, humans also make errors in research. How do AI systems compare in terms of error rates or susceptibility to bias? Are there areas where AI could complement human limitations?\n- Table 2 is not referenced in the main text. It is unclear where the numbers originate from and how the \"impact level\" is defined. This raises concerns about transparency and correctness of the paper.\n- Figure 1 occupies significant space but it's duplicated content described in Section 2.2 and also includes blurred text. Its value to the reader is limited and space could be used more wisely.\n- The paper places too much emphasis on the Sakana AI case. A more diverse set of case studies would make the safety risks appear less anecdotal and more generalizable."
      },
      "limitations": {
        "value": "The paper does not clearly state its methodology for literature selection. Since this is positioned as a literary review paper, it would be important to include a limitations section that discusses how the reviewed papers were chosen, which criteria were used, and what may have been excluded."
      },
      "overall": {
        "value": 2
      },
      "confidence": {
        "value": 4
      },
      "ethical_concerns": {
        "value": "N/A"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission133/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759512900156,
    "mdate": 1760632176118,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_SJQT"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission133/Reviewer_SJQT"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "LkcSAfplrd",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "decision": {
        "value": "Reject"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! We regret to inform you that your submission has not been accepted. Please see the reviews below for more information."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission133/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948927867,
    "mdate": 1760632275981,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "L5Rasv3BnS",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- No systematic survey methodology (no search strategy, inclusion/exclusion criteria, screening, or bias assessment).\n- Heavy reliance on gray/non-peer-reviewed sources for core empirical claims (e.g., [6], [9], [14], [17]).\n- Tables 1 (page 3) and 2 (page 4) present quantitative figures without clear definitions, sources, or statistical context (sample sizes, CIs).\n- Mixed, non-comparable evaluation contexts (different tasks and domains) summarized as if directly comparable (Table 1).\n- Unclear or potentially inaccurate attribution of specific behaviors/statistics to particular models; insufficiently supported quotes.\n- Internal inconsistency: checklist claims reproducibility ([Yes], page 13) vs. absence of a reproducible review protocol.\n- Formal non-compliance: instruction blocks in the Agents4Science checklists were not removed (pages 10–15).\n- Overgeneralization from anecdotal or single-source reports; limited triangulation.\n- Lack of operational definitions for key constructs (e.g., hallucination, research fabrication) impedes valid comparison.\n- No uncertainty quantification or sensitivity analysis for reported rates; no meta-analytic synthesis."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776682801,
    "mdate": 1760640260443,
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "DjS7VVbPVw",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper surveys safety issues for “AI scientist” systems, introducing a four-part taxonomy (technical safety, research integrity, dual-use, alignment), three case studies, and technical/governance recommendations. The topic is timely and the manuscript is well-structured and readable. Strengths include clear articulation of risk categories, useful consolidation of safety concerns, concrete case studies, and actionable recommendations. However, the evidentiary basis and methodology are insufficient for a top venue survey: quantitative claims lack rigorous support and comparability, with many relying on gray literature rather than peer-reviewed sources. Central citations are often to blogs or news articles, and some claims are not anchored to reproducible empirical work. The promised evaluation framework is not concretely delivered, and the automation bias case study lacks specificity to scientific agents. Methodological details (literature inclusion, search strategy, criteria) are missing, and some language is overstated relative to the evidence. The taxonomy is not clearly novel, and the paper does not provide new empirical evidence or a substantially new analytical framework. Reproducibility is limited by the absence of a transparent, systematic methodology. There is no dedicated Limitations section, and the lack of systematic review protocol and reliance on gray literature limit the paper’s impact. Important peer-reviewed work is missing or underemphasized. Actionable suggestions include establishing a systematic survey methodology, calibrating and sourcing quantitative claims, replacing gray-literature citations, delivering a concrete evaluation framework, adding a Limitations section, deepening case studies with traceable data, and refining governance recommendations. Overall, the paper is timely and potentially valuable, but not yet suitable for acceptance due to weak evidentiary grounding, lack of systematic methodology, and absence of the promised evaluation framework. Methodological rigor and grounding claims in peer-reviewed evidence are needed to improve the paper."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission133/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775347668,
    "mdate": 1760632175967,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "7bPKcF3dTG",
    "forum": "7bPKcF3dTG",
    "content": {
      "title": {
        "value": "AI Scientist Safety Issues: A Comprehensive Survey and Analysis"
      },
      "keywords": {
        "value": [
          "AI scientist; agents; LLM agents; LLM; safety"
        ]
      },
      "abstract": {
        "value": "The emergence of autonomous AI systems capable of conducting scientific research has introduced unprecedented opportunities and risks in the scientific enterprise. This survey examines the current state of AI scientist safety, analyzing major safety concerns, documented incidents, and emerging challenges in automated scientific discovery systems. Through comprehensive literature review and case study analysis, we identify four critical risk categories: technical failures and hallucinations, dual-use and misuse potential, research integrity violations, and autonomous system alignment problems. Our analysis reveals that while current AI systems like GPT-4, Claude, and specialized research agents demonstrate remarkable capabilities, they exhibit concerning failure modes including systematic hallucination rates (1.7-33\\%), research fabrication, and dangerous autonomous behaviors. We propose a framework for evaluating AI scientist safety and provide recommendations for safer deployment of automated research systems."
      },
      "pdf": {
        "value": "/pdf/97256d71a650bd060890e558f9c9fe7137a0090b.pdf"
      },
      "venue": {
        "value": "Submitted to Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference/Rejected_Submission"
      },
      "_bibtex": {
        "value": "@misc{\nanonymous2025ai,\ntitle={{AI} Scientist Safety Issues: A Comprehensive Survey and Analysis},\nauthor={Anonymous},\nyear={2025},\nurl={https://openreview.net/forum?id=7bPKcF3dTG}\n}"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1757788300897,
    "odate": 1758112145415,
    "mdate": 1759960937703,
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission133/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "2HT92Oh0k0",
    "forum": "7bPKcF3dTG",
    "replyto": "7bPKcF3dTG",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nPlease look at your references to confirm they are good.\n\n**Examples of references that could not be verified (they might exist but the automated verification failed):**\n\n- Gpt hallucinating entire research studies by OpenAI Developer Community\n- 11 famous ai disasters by Mary K. Pratt\n- Responsible ai in biotechnology: Balancing discovery, innovation and biosecurity risks by Robert Wilson, Katherine Lee, and Michael Brown"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777718231,
    "mdate": 1760640259763,
    "signatures": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission133/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]