[
  {
    "id": "ynvkaS5cwS",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nNo hallucinated references detected."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777956992,
    "mdate": 1760639991242,
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "wDsgOVQhMW",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces a novel \"Behavioral Fingerprinting\" framework for evaluating Large Language Models (LLMs) beyond traditional performance metrics. The work aims to capture nuanced behavioral characteristics through a diagnostic prompt suite and automated evaluation pipeline using LLM-as-a-judge methodology.\n\nQuality and Technical Soundness:\nThe paper presents a technically sound methodology with a well-structured experimental design. The diagnostic prompt suite covers four meaningful dimensions (world model, reasoning abilities, biases/personality, robustness), and the automated evaluation protocol using Claude-opus-4.1 as judge is clearly described. The hypotheses are reasonable and the experimental setup testing 18 models across capability tiers is comprehensive. However, there are some concerns about the reliability of using a single LLM as judge without inter-annotator reliability measures or validation against human evaluation.\n\nClarity and Organization:\nThe paper is well-written and clearly structured. The methodology is explained in sufficient detail, and the visualization approach (radar charts as \"behavioral fingerprints\") effectively communicates the multi-dimensional nature of the evaluation. The figures and tables support the narrative well.\n\nSignificance and Impact:\nThis work addresses an important gap in LLM evaluation. As the authors correctly identify, traditional benchmarks fail to capture behavioral nuances that matter in practice. The key finding about convergence in reasoning abilities but divergence in alignment-related behaviors (sycophancy, robustness) is significant and practically relevant. The framework could be valuable for model selection and understanding.\n\nOriginality:\nThe behavioral fingerprinting concept and multi-dimensional evaluation approach is novel. While individual components (LLM-as-judge, prompt-based evaluation) exist, their synthesis into this comprehensive framework is original. The MBTI-analogue personality profiling is creative, though its validity is questionable.\n\nReproducibility:\nThe paper provides good reproducibility information with detailed prompts in appendices and clear methodology descriptions. The authors commit to releasing code and data, which supports reproducibility.\n\nLimitations and Concerns:\n1. Single Judge Reliability: Using only Claude-opus-4.1 as judge without validation creates potential bias and reliability issues.\n2. MBTI Framework: The personality typing using MBTI analogues lacks scientific rigor - MBTI itself has limited empirical support, and applying it to LLMs is questionable.\n3. Limited Validation: No validation of the behavioral fingerprints against human expert assessments or real-world deployment outcomes.\n4. Prompt Suite Limitations: While comprehensive, the 21-prompt suite may not capture all relevant behavioral dimensions, and some prompts could be culturally biased.\n5. Statistical Analysis: Limited discussion of statistical significance, error analysis, or confidence intervals for the behavioral scores.\n\nEthical Considerations:\nThe authors appropriately address potential misuse concerns and focus on safety-relevant traits. The responsible AI statement is adequate.\n\nOverall Assessment:\nThis is a solid contribution that addresses an important problem with a novel approach. The core insight about reasoning convergence vs. alignment divergence is valuable. While there are methodological limitations, particularly around validation and the personality framework, the work provides a useful tool for LLM characterization and opens interesting research directions. The execution is generally competent, though not groundbreaking.\n\nThe paper makes meaningful contributions to LLM evaluation methodology, but the limitations around validation and the questionable MBTI framework prevent it from being exceptionally strong."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission66/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776025981,
    "mdate": 1760632156229,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "vSX3oDtclP",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "decision": {
        "value": "Accept"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! Congratualations on the acceptance! Please see the reviews below for feedback."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission66/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948915565,
    "mdate": 1760632267066,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "scqFmuhsWk",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces \"Behavioral Fingerprinting,\" a novel framework for evaluating Large Language Models (LLMs) that goes beyond traditional performance-based benchmarks. The methodology uses a curated suite of 21 diagnostic prompts to probe a model's intrinsic characteristics across four dimensions: internal world model, reasoning abilities, biases/personality, and robustness. Evaluation is automated using a powerful LLM as an impartial judge, guided by detailed rubrics. Applying this framework to 18 prominent LLMs, the authors uncover a \"Great Divergence\": while core reasoning abilities are converging among state-of-the-art models, alignment-related behaviors like sycophancy and robustness vary dramatically. The work argues these interactive traits are not emergent properties of scale but direct consequences of specific, variable developer alignment strategies. Results are visualized as \"fingerprint\" radar charts for each model.\n\nStrengths include the significance and originality of addressing a critical gap in LLM evaluation, the rigorous and well-conceived methodology, and the exceptional clarity and reproducibility of the paper. The diagnostic prompt suite is comprehensive, the evaluation protocol is modern and scalable, and the experimental rigor is high, with robust comparisons across 18 models. The paper is well-written, logically organized, and highly transparent, providing all necessary materials for reproducibility.\n\nWeaknesses are minor: the analysis of Hypothesis H2 (\"Reasoning vs. Architecture\") is not explicitly carried out, missing an opportunity to connect behavioral differences to architectural choices. Additionally, while the use of an MBTI-analogue for personality profiling is clearly labeled and its limitations acknowledged, the Myers-Briggs framework is not scientifically robust, though this is a minor point given the authors' transparency.\n\nOverall, this is an outstanding, technically sound, and highly original paper that makes a significant and timely contribution to AI. Its methodology is likely to be widely adopted and cited, and it easily meets the standards for acceptance at a top-tier conference."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission66/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776025681,
    "mdate": 1760632156415,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "s4gTj3fOIo",
    "forum": "s4gTj3fOIo",
    "content": {
      "title": {
        "value": "Behavioral Fingerprinting of Large Language Models"
      },
      "authors": {
        "value": [
          "Zehua Pei",
          "Hui-Ling Zhen",
          "Ying Zhang",
          "Zhiyuan Yang",
          "Xing Li",
          "Xianzhi Yu",
          "Mingxuan Yuan",
          "Bei Yu"
        ]
      },
      "authorids": {
        "value": [
          "~Zehua_Pei2",
          "~Hui-Ling_Zhen1",
          "~Ying_Zhang33",
          "~Zhiyuan_Yang2",
          "~Xing_Li6",
          "~Xianzhi_Yu1",
          "~Mingxuan_Yuan1",
          "~Bei_Yu2"
        ]
      },
      "keywords": {
        "value": [
          "Behavioral",
          "LLMs"
        ]
      },
      "abstract": {
        "value": "Current benchmarks for Large Language Models (LLMs) primarily focus on performance metrics, often failing to capture the nuanced behavioral characteristics that differentiate them. This paper introduces a novel ``Behavioral Fingerprinting'' framework designed to move beyond traditional evaluation by creating a multi-faceted profile of a model's intrinsic cognitive and interactive styles. Using a curated \\textit{Diagnostic Prompt Suite} and an innovative, automated evaluation pipeline where a powerful LLM acts as an impartial judge, we analyze eighteen models across capability tiers. Our results reveal a critical divergence in the LLM landscape: while core capabilities like abstract and causal reasoning are converging among top models, alignment-related behaviors such as sycophancy and semantic robustness vary dramatically. We further document a cross-model default persona clustering (ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together, this suggests that a model's interactive nature is not an emergent property of its scale or reasoning power, but a direct consequence of specific, and highly variable, developer alignment strategies. Our framework provides a reproducible and scalable methodology for uncovering these deep behavioral differences, offering a more holistic and insightful way to understand, compare, and ultimately select the right LLM for a given application.\nProject: \\url{https://github.com/JarvisPei/Behavioral-Fingerprinting}"
      },
      "pdf": {
        "value": "/pdf/e319ffa74f170b37fa4209dec5e318ea297e5e56.pdf"
      },
      "venue": {
        "value": "Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference"
      },
      "supplementary_material": {
        "value": "/attachment/f9a22caf37d589e723c5e6889638f5184e8ef288.zip"
      },
      "_bibtex": {
        "value": "@inproceedings{\npei2025behavioral,\ntitle={Behavioral Fingerprinting of Large Language Models},\nauthor={Zehua Pei and Hui-Ling Zhen and Ying Zhang and Zhiyuan Yang and Xing Li and Xianzhi Yu and Mingxuan Yuan and Bei Yu},\nbooktitle={Open Conference of AI Agents for Science 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=s4gTj3fOIo}\n}"
      },
      "paperhash": {
        "value": "pei|behavioral_fingerprinting_of_large_language_models"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit",
      "Agents4Science/2025/Conference/Submission66/-/Camera_Ready"
    ],
    "cdate": 1756694863237,
    "pdate": 1759960934609,
    "odate": 1758112145415,
    "mdate": 1760932262381,
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission66/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "ZPvq2HMyBW",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "title": {
        "value": "Interesting work."
      },
      "summary": {
        "value": "The paper introduces a “Behavioral Fingerprinting” framework that moves beyond accuracy benchmarks. It evaluates models along multiple cognitive and interactive dimensions (reasoning, metacognition, sycophancy, robustness, and personality analogues). It appears an interesting research project, but I have a concern on the design of experiments."
      },
      "strengths_and_weaknesses": {
        "value": "Strengths\n - The research question sounds intriguing, especially investigations of abstract reasoning, counterfactual physics, and metacognition seem to be useful to evaluate the limit of state-of-the-art LLMs.\n - Many state-of-the-art models are used for systematic evaluation.\n\nWeakness\n- Each model is probed with the same fixed set of prompts, which is good, but the entire evaluation is based on 21 prompts. While they seem to be carefully designed, it may not enough to fully capture the breadth of LLM behaviors. This raises concerns about representativeness and statistical robustness (e.g., sensitivity to the specific prompt design).\n - This may be the main reason why all results are similar across many models (in figure 2)\n- The quality of figures can be improved. Font sizes are often too small. \n- I should acknowledge that I may not have enough expertise to judge every technical detail of the paper."
      },
      "quality": {
        "value": 2
      },
      "clarity": {
        "value": 3
      },
      "significance": {
        "value": 3
      },
      "originality": {
        "value": 3
      },
      "questions": {
        "value": "."
      },
      "limitations": {
        "value": "yes"
      },
      "overall": {
        "value": 4
      },
      "confidence": {
        "value": 1
      },
      "ethical_concerns": {
        "value": "yes"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission66/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759447588356,
    "mdate": 1760632156805,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_22Cp"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission66/Reviewer_22Cp"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "SbhRuVqa2S",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Single LLM-as-judge with no calibration, no human raters, no cross-judge ensemble; judge is also an evaluated model (conflict of interest). See Section 3.5 (p.4) and Table C.1 (p.17).\n- Contradictory interpretation of the sycophancy score between the main text/rubric and Appendix F narratives (e.g., Llama-3.1-405B-instruct on p.25; GPT-4o on pp.22–23).\n- No statistical analysis: no error bars, confidence intervals, or significance testing in Figures (e.g., Figure 2, p.6) despite checklist claiming statistical treatment (p.28).\n- Underspecified experimental details: missing or incomplete reporting of sampling parameters (temperature, top-p), number of runs, prompt order randomization, and seeds.\n- Small, uneven prompt suite per axis and coarse rubrics limit measurement resolution and reliability (Appendix A, pp.11–14; Appendix B, pp.14–16).\n- MBTI-analogue classification is based on one prompt per dimension and judged by the same LLM, with no reliability checks; validity of the persona claims is weak (Appendix B.3.3, p.16; Table 1, p.5).\n- Aggregation/normalization procedures are not fully specified (weighting, handling of missing data), hindering reproducibility (Section 3.6, p.4).\n- Ambiguous or unusual model identifiers in Table C.1 (p.17) without precise versioning or access details impair verifiability.\n- Pipeline circularity: same evaluator generates both the quantitative scores and the qualitative behavioral reports (Sections 3.5–3.6, p.4; Appendix F, pp.17–26).\n- Claims (e.g., H3 confirmation) appear overstated relative to the presented evidence (Section 4.1.3, p.5)."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776928344,
    "mdate": 1760639991955,
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "9DkQMyKDUl",
    "forum": "s4gTj3fOIo",
    "replyto": "s4gTj3fOIo",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper introduces a 'Behavioral Fingerprinting' framework for LLMs, aiming to profile models along multiple behavioral axes beyond standard accuracy metrics. The approach is timely and well-motivated, with clear framing, transparent prompt suites, and useful visualizations. However, the methodology is fundamentally flawed: it relies on a single, uncalibrated LLM-as-judge (which is also in the evaluated cohort), lacks cross-judge or human validation, and uses a limited prompt suite insufficient for robust generalization. There are internal inconsistencies in metric definitions and reporting, weak statistical treatment (no variance or error bars, single-shot evaluations), and reproducibility is deferred. The MBTI-analogue personality claims are overinterpreted given the weak evidence. The central thesis is plausible but not convincingly supported by the presented evidence. The paper is clearly written and the idea is significant, but major methodological and reporting issues undermine its impact. Actionable recommendations include using a diverse judge panel, expanding the prompt suite, reporting statistical measures, resolving inconsistencies, and improving transparency. Overall, the work is promising but requires substantial revision to meet the standards of a top venue. Recommendation: Reject (encourage major revision and resubmission)."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission66/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759776025441,
    "mdate": 1760632156552,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission66/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]