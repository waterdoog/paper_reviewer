[
  {
    "id": "sMnRJUYu4B",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Decoding and randomness control not specified (temperature/top_p/seed), and no repeated runs to assess variance; results may be sensitive to stochastic decoding.\n- Single paper and single error type; no evaluation on controls for false positive rates despite the framework supporting such metrics.\n- Prompt inconsistency: quoted prompt (Section 3.1) lacks explicit instructions to cite locations/recommend fixes, yet detection criteria require referencing both sections (Section 3.2).\n- Provider/model identification ambiguity: mismatch between listed API providers and evaluated model families; unclear endpoints for DeepSeek/Meta-Llama; unusual model name 'openai gpt-oss-120b' reduces reproducibility.\n- Family-level statements (e.g., 'detection was perfect within Cohere and Gemini') are based on n=2 per family and a single document; although caveated, this can be misinterpreted without repeated trials.\n- Framework mentions broader metrics (precision/recall, over-flagging) but the results section reports only binary detection; consider reporting false positive rates and coverage on control (no-error) documents."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776877000,
    "mdate": 1760640137855,
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "qNpjB1Qr2p",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper introduces RefereeSim, a reproducible framework to evaluate AI reviewers on manuscripts with seeded, known errors. The proof-of-concept injects a sample-size inconsistency and tests 11 LLMs; only 4 detect the error, with a 36.4% detection rate. The framework is modular, open-source, and emphasizes reproducibility. Strengths include a clear task, sensible architecture, and strong reproducibility. Weaknesses are a very narrow evaluation (one paper, one error type), lack of baselines, no ablation studies, incomplete inference reporting, limited analysis, and no real-paper tests. The paper is technically sound and clearly written, but the experimental validation is too limited for strong conclusions. The framework could be significant if expanded. Suggestions include adding baselines, expanding tasks, reporting more metrics, and including real-paper evaluations. Overall, it's a clean proof-of-concept but needs broader experiments and analyses for a higher evaluation."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 3
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 3
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission101/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775878015,
    "mdate": 1760632165480,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "qK4OjjMaWA",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "title": {
        "value": "Human Review"
      },
      "summary": {
        "value": "This paper introduces RefereeSim, a proof-of-concept evaluation framework for assessing AI models' ability to detect errors in scientific manuscripts. The authors generate a synthetic research paper with a deliberately seeded error. In this initial study, they test whether 11 production LLMs across five model families can identify a simple sample-size inconsistency between the abstract (n=2068) and methods section (n=1991). Only 4 of 11 models (36.4%) successfully detected the discrepancy. Successful models explicitly compared numbers across sections and stated the inconsistency clearly. The authors propose this modular, reproducible platform as a foundation for systematically evaluating AI reviewer capabilities."
      },
      "strengths_and_weaknesses": {
        "value": "Strengths:\n- The methodology is sound with a clear evaluation protocol. The modular architecture is well-designed and reproducible. The scoring criteria are unambiguous and appropriate for the task.\n- The paper is well-organized and written clearly. The seeded error is documented in the appendix, and the evaluation criteria are transparent.\n- The ground-truth error seeding approach offers a valuable alternative to existing evaluation methods that rely on subjective rubrics or human preferences. The systematic comparison across 11 models from multiple vendors provides useful empirical data.\n\nWeaknesses:\n- The scope is extremely narrow, with only one synthetic paper that has one error. This severely limits the generalizability of the 36.4% detection rate. The task itself (finding a numeric mismatch) is relatively trivial compared to substantive review dimensions like validating scientific claims, assessing novelty, evaluating whether conclusions follow from results, or checking literature coverage. The authors acknowledge this but don't adequately address why this simple task should be prioritized.\n- There are no ablations on prompt design. The design implications (especially requiring location citations) could have been trivially tested in the original experimental setup rather than presented as post-hoc recommendations. The paper lacks evidence that their suggested improvements actually work.\n- The choice to use synthetic papers rather than real manuscripts is not justified. Real papers would better capture domain jargon, incomplete reporting, and the messiness that might affect model performance.\n- The impact is limited by the proof-of-concept nature. While the platform has potential, the current results don't demonstrate that the framework can scale to more complex, realistic review scenarios. The paper doesn't show that detecting typos and numeric mismatches translates to LLM capability on higher-order review tasks that actually matter for scientific quality control."
      },
      "quality": {
        "value": 2
      },
      "clarity": {
        "value": 3
      },
      "significance": {
        "value": 1
      },
      "originality": {
        "value": 2
      },
      "questions": {
        "value": "See weaknesses"
      },
      "limitations": {
        "value": "See weaknesses"
      },
      "overall": {
        "value": 3
      },
      "confidence": {
        "value": 4
      },
      "ethical_concerns": {
        "value": "No"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission101/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759189978490,
    "mdate": 1760632165748,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_JYXP"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission101/Reviewer_JYXP"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "pRJKWzBZ70",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nNo hallucinated references detected."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777749567,
    "mdate": 1760640137173,
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "QXyeIJ9PQ3",
    "forum": "QXyeIJ9PQ3",
    "content": {
      "title": {
        "value": "RefereeSim: A Proof-of-Concept Evaluation Framework for AI-Powered Scientific Paper Reviewers"
      },
      "keywords": {
        "value": [
          "AI Reviewer",
          "Automated error detection",
          "Research Evaluation methodologies"
        ]
      },
      "TLDR": {
        "value": "We introduce RefereeSim, a lightweight evaluation platform that stress-tests AI “reviewers” with synthetic papers in which errors are deliberately seeded un- der full ground truth."
      },
      "abstract": {
        "value": "Motivation. Scientific peer review is under pressure from ever–growing sub-\nmission volumes and long delays, while the capabilities of large language mod-\nels (LLMs) invite the question: can AI reliably assist reviewers? Approach.\nWe introduce RefereeSim, a lightweight evaluation platform that stress-tests AI\n“reviewers” with synthetic papers in which errors are deliberately seeded un-\nder full ground truth. This proof-of-concept study injects a single, concrete\ninconsistency—a sample-size misreport between the abstract (2068) and the meth-\nods (1991)—and asks 11 production LLMs spanning five model families to re-\nview the paper under identical prompts. Findings. Only 4 of 11 models (36.4%)\nidentified the discrepancy. Detection was perfect within the Cohere (2/2) and\nGemini (2/2) families, and absent for DeepSeek (0/3), Llama (0/3), and the eval-\nuated OpenAI model (0/1). Successful models (i) explicitly compared numbers\nacross sections, (ii) stated the inconsistency, and (iii) recommended correction.\nContributions. (1) A transparent, reproducible evaluation pipeline that aligns\nreviewer outputs with seeded ground truth; (2) a first multi-vendor snapshot on\na core consistency task; and (3) actionable guidance for building AI-assisted re-\nviewing workflows. Implications. Even under favorable, controlled conditions,\nmany models miss basic cross-section consistency checks, underscoring the need\nfor structured reasoning passes and human oversight before deployment in peer\nreview."
      },
      "pdf": {
        "value": "/pdf/d9202a051590f8115544ac1e259d41c3be25f298.pdf"
      },
      "venue": {
        "value": "Submitted to Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference/Rejected_Submission"
      },
      "_bibtex": {
        "value": "@misc{\nanonymous2025refereesim,\ntitle={RefereeSim: A Proof-of-Concept Evaluation Framework for {AI}-Powered Scientific Paper Reviewers},\nauthor={Anonymous},\nyear={2025},\nurl={https://openreview.net/forum?id=QXyeIJ9PQ3}\n}"
      },
      "supplementary_material": {
        "value": "/attachment/ca0e4e61995384bb631331e2ee641f32d95cdee1.pdf"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/Submission101/-/Revision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1757570844651,
    "odate": 1758112145415,
    "mdate": 1759960935947,
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission101/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "PsAxEGbEVj",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces RefereeSim, a framework for evaluating AI models' ability to detect errors in scientific papers using synthetic manuscripts with seeded errors. The work is technically sound, with a clearly described modular methodology and appropriate experimental design for a proof-of-concept study. The scoring criteria are objective, and the authors are transparent about the limitations of testing only one error type on one synthetic paper. The paper is well-written, organized, and provides sufficient technical detail for reproduction, including code, data, and hardware specifications. The findings are significant, showing that only 36.4% of models detected a basic consistency error, and the behavioral analysis offers actionable insights for system design. The approach is novel, particularly the controlled error seeding methodology with full ground truth, and the multi-model comparison provides new empirical insights. Reproducibility is excellent, with complete code and documentation provided. The authors thoughtfully address ethical considerations and limitations, and the related work section is appropriate. Strengths include the novel methodology, clear empirical findings, excellent reproducibility, and honest discussion of limitations. Weaknesses are the limited scope, lack of human reviewer comparison, missing analysis of model family performance differences, and absence of prompt variation testing. Overall, the paper makes a solid contribution to understanding AI capabilities in scientific review through a novel, reproducible evaluation framework, with significant findings for the AI-assisted peer review community despite its intentionally narrow scope."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission101/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775878425,
    "mdate": 1760632165062,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "GSWD68HKfc",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "decision": {
        "value": "Reject"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! We regret to inform you that your submission has not been accepted. Please see the reviews below for more information."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission101/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948929461,
    "mdate": 1760632271381,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "BbIYqWJa58",
    "forum": "QXyeIJ9PQ3",
    "replyto": "QXyeIJ9PQ3",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces RefereeSim, a proof-of-concept framework for evaluating the capabilities of Large Language Models (LLMs) as scientific paper reviewers. The framework generates synthetic manuscripts with controlled errors to create a reliable ground truth for assessment. In an initial experiment, 11 production LLMs were tested on their ability to detect a mismatch in reported sample size between the abstract and methods section; only 4 of 11 models (36.4%) identified the error. The analysis found that successful models performed explicit cross-referencing, while others gave generic feedback. The paper offers actionable design principles for AI-assisted reviewing tools and a roadmap for extending RefereeSim.\n\nStrengths include the significance and timeliness of the work, methodological soundness and originality, exceptional clarity and organization, exemplary reproducibility (with open-source code and data), insightful analysis with actionable recommendations, and an honest discussion of limitations. The main limitation is the narrow experimental scope (one error type, one synthetic paper), but this is acknowledged and justified as appropriate for a proof-of-concept. The paper establishes a strong methodology and a clear path for future expansion.\n\nOverall, this is an outstanding, technically flawless paper that addresses a significant problem with a novel and robust methodology, and is presented with exceptional clarity and transparency. It is highly recommended for acceptance and has the potential to be a landmark paper in the field."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission101/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775878245,
    "mdate": 1760632165258,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission101/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]