[
  {
    "id": "zC8YfwEDCW",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nNo hallucinated references detected."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759778000927,
    "mdate": 1760640149542,
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "tsDcIz2gS1",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper introduces UnitMath, a rule-based, unit-aware numerical reasoning framework for scientific table-claim verification, emphasizing explicit unit handling, interpretable reasoning traces, and stress tests for unit rescaling and percentage-type sensitivity. Strengths include a focus on unit semantics and dimensional consistency, a sensible priority-based design, and valuable stress-test framing. However, the paper lacks critical methodological details (unit extraction, dimensional checks, operationalization of fold-changes), is limited to a single dataset (SciTab), and underspecifies evaluation protocols and baselines. Some internal inconsistencies are noted, such as the impact of percentage conversion and the mapping of refusals to final labels. The writing is clear but omits reproducibility-relevant specifics, and the reported F1 (54.1) is moderate and below some baselines. The originality lies in the holistic framing rather than the underlying techniques. While code and data are claimed to be released, the paper lacks sufficient detail for full reproducibility. Ethics and limitations are candidly discussed, but some citations are misaligned or duplicated. Actionable suggestions include providing rigorous methodological details, expanding evaluation, and clarifying baselines. Overall, the paper addresses an important problem with a promising approach, but missing methodological rigor and evaluation transparency prevent a recommendation for acceptance at this time."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 3
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 3
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission122/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775930177,
    "mdate": 1760632171602,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "lUt3jSTemA",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Misuse of statistical terminology: calling a fixed 5% relative-difference threshold \"statistical significance\" (page 4, lines 148–151).\n- Internal inconsistency in reported accuracies: Section 4.4 reports 26–40% accuracies by reasoning priority and \"33.6% overall\" (page 5) that contradict main results (~54.6% accuracy, page 5); methodology for these figures is not explained.\n- Baseline comparisons (Table 1, pages 6–7) lack methodological details (prompting, shots, decoding, evaluation protocol), and several baseline numbers/citations appear questionable (e.g., Vicuna results and citation [13]).\n- Bibliographic inaccuracies: [11] appears unrelated to units/ontologies; [38] and [39] duplicate; some references do not correspond to the claimed evaluations.\n- Under-specified unit/dimensional implementation: no clear description of how units are parsed, normalized, and checked against an ontology; reliance appears to be on regex without demonstrated integration of QUDT/OM/UCUM.\n- Stress-test methodology under-specified: no sample sizes, construction details, annotation, or baseline selection; yet headline comparisons (e.g., 94% vs 67%) are emphasized.\n- No calibration or sensitivity analysis for heuristic thresholds (tolerances, fuzzy matching, relative-difference criteria) despite claims of confidence-based decisions.\n- Ablation omits a direct switch for the key \"percentage points vs percent\" disambiguation, so the contribution of that capability is not isolated.\n- Lack of statistical uncertainty reporting (no error bars/confidence intervals) and limited dataset scope (SciTab only) weakens generality claims."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776895579,
    "mdate": 1760640150203,
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "ihko9qii9m",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "title": {
        "value": "review"
      },
      "summary": {
        "value": "This paper introduces UnitMath, a rule-based framework for unit-aware numerical reasoning in scientific table-claim verification. The motivation is that current table-based fact verification systems, even large LLMs like GPT-4, often make basic numeric and dimensional errors (e.g., confusing percent vs. percentage points or comparing incompatible units). The authors propose UnitMath, a priority-based reasoning cascade. On the SciTab dataset, UnitMath achieves 54.1% macro-F1, outperforming prior non-neural and mid-sized neural baselines. The authors emphasize interpretability, stress tests for rescaling and percentage-type sensitivity, and reproducibility over raw scale or accuracy."
      },
      "strengths_and_weaknesses": {
        "value": "Strength: motivation and problem significance\n\nWeaknesses: the methods are not clearly presented, it's described narratively rather than algorithmically."
      },
      "quality": {
        "value": 3
      },
      "clarity": {
        "value": 2
      },
      "significance": {
        "value": 2
      },
      "originality": {
        "value": 3
      },
      "questions": {
        "value": "The methods are described narratively rather than algorithmically. Key implementation aspects remain unclear:\n1. How are numerical mentions aligned between claims and tables?\n2. What ontology or mapping is used for units (UCUM, QUDT?)?\n3. How are “dimensional consistency checks” implemented programmatically?\n4. What determines the confidence scores (0.6, 0.75, etc.) — heuristic thresholds or data-driven calibration?\n5. While the authors frame the system as principled, many thresholds (e.g., “within 2% relative error,” “fuzzy match threshold = 0.7”) are arbitrary. Can the authors explain how these numbers are chosen?"
      },
      "limitations": {
        "value": "same as above"
      },
      "overall": {
        "value": 4
      },
      "confidence": {
        "value": 3
      },
      "ethical_concerns": {
        "value": "NA"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission122/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759718341815,
    "mdate": 1760632171846,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AAyc"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission122/Reviewer_AAyc"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "gbevYET9KU",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces UnitMath, a rule-based framework for unit-aware numerical reasoning in scientific table-claim verification. The technical approach is sound and addresses a genuine problem in scientific fact-checking, focusing on handling units, percentages, and dimensional consistency. The methodology, while appropriate, is not particularly novel, relying on regex-based extraction, rule-based unit conversion, and a priority-based reasoning cascade. Evaluation on SciTab shows competitive performance (54.1% macro F1) against neural baselines, with well-designed stress tests for unit rescaling and percentage-type sensitivity. The paper is clearly written and organized, with compelling motivation and systematic presentation, though some implementation details are lacking. The work is significant for its interpretability and systematic error prevention, but its impact is limited by the rule-based nature, single dataset evaluation, and lower performance compared to state-of-the-art models like GPT-4. The originality lies in the combination of known techniques for explicit unit-aware reasoning and stress testing. Reproducibility is supported by the inclusion of code and data, though some heuristics are under-documented. Major strengths include addressing a real problem, interpretability, stress tests, error prevention, and modularity. Weaknesses include limited evaluation, performance gap, generalization issues, reliance on regex/manual tuning, and English-centric design. Technical issues involve minimal ablation impact, underspecified implementation details, and limited stress test scope. Overall, the paper is technically solid and practically valuable, but primarily an engineering contribution with limited research significance due to scalability and generalization constraints."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 3
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 3
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission122/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775930676,
    "mdate": 1760632171066,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "OG8sFxeNHv",
    "forum": "OG8sFxeNHv",
    "content": {
      "title": {
        "value": "UnitMath: Unit-Aware Numerical Reasoning and Dimensional Consistency for Scientific Table Claims"
      },
      "authors": {
        "value": [
          "Xanh Ho",
          "Tian Cheng Xia",
          "Khoa Duong",
          "Yun-Ang Wu",
          "Ha-Thanh Nguyen",
          "Akiko Aizawa"
        ]
      },
      "authorids": {
        "value": [
          "~Xanh_Ho1",
          "~Tian_Cheng_Xia1",
          "~Khoa_Duong1",
          "~Yun-Ang_Wu2",
          "~Ha-Thanh_Nguyen1",
          "~Akiko_Aizawa1"
        ]
      },
      "keywords": {
        "value": [
          "claim verification",
          "table-based claim",
          "numerical reasoning"
        ]
      },
      "TLDR": {
        "value": "Unit-Aware Numerical Reasoning"
      },
      "abstract": {
        "value": "Recent progress in table-based fact verification has improved semantic understanding of schema and cell content, but models still stumble on quantitative claims that hinge on units and dimensional constraints. Errors arise when systems conflate percent with percentage points, treat fold changes as plain ratios, or compare quantities across incompatible dimensions, leading to brittle and untrustworthy decisions. We introduce UnitMath, a unit-aware numerical reasoning framework specifically designed for scientific table-claim verification. Our approach combines: (i) enhanced numerical extraction with comprehensive pattern matching for percentages, decimals, and fractions, (ii) robust unit-aware verification with automatic percentage-decimal conversion and tolerance-based matching, and (iii) structured reasoning traces that capture complete decision pathways for interpretability. UnitMath achieves 54.1\\% macro F1 on SciTab, demonstrating competitive performance through principled design rather than parameter scaling. Key advantages include: \\textbf{explainable reasoning} with full traceability of numerical comparisons, textbf{lightweight architecture} requiring no neural training, textbf{modular design} enabling drop-in integration with existing table encoders, and \\textbf{systematic error prevention} for unit-related failures that plague larger models. The framework provides comprehensive stress testing for unit rescaling invariance and percentage-type sensitivity, validating true unit understanding rather than surface pattern matching. This work establishes unit-aware reasoning as a valuable complement to scaling-based approaches in scientific domains where numerical precision and interpretability are paramount."
      },
      "pdf": {
        "value": "/pdf/bc69b91c7a10c9a3c664065fe3bb7c11d3114882.pdf"
      },
      "supplementary_material": {
        "value": "/attachment/9d84f6eaac6efe11ae5e1b6ca6ffcaf4c2523a24.zip"
      },
      "venue": {
        "value": "Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference"
      },
      "_bibtex": {
        "value": "@inproceedings{\nho2025unitmath,\ntitle={UnitMath: Unit-Aware Numerical Reasoning and Dimensional Consistency for Scientific Table Claims},\nauthor={Xanh Ho and Tian Cheng Xia and Khoa Duong and Yun-Ang Wu and Ha-Thanh Nguyen and Akiko Aizawa},\nbooktitle={Open Conference of AI Agents for Science 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=OG8sFxeNHv}\n}"
      },
      "paperhash": {
        "value": "ho|unitmath_unitaware_numerical_reasoning_and_dimensional_consistency_for_scientific_table_claims"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/Submission122/-/Revision",
      "Agents4Science/2025/Conference/-/Edit",
      "Agents4Science/2025/Conference/Submission122/-/Camera_Ready"
    ],
    "cdate": 1757668362968,
    "pdate": 1759960937072,
    "odate": 1758112145415,
    "mdate": 1760940064665,
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission122/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "CXmpQyAROu",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces UnitMath, a non-neural, rule-based framework for scientific table-claim verification. The work is motivated by the critical observation that even large-scale language models frequently fail on quantitative reasoning tasks that require understanding of units, dimensional constraints, and the distinction between concepts like percentages and percentage points. The proposed system, UnitMath, employs a priority-based reasoning cascade to deliver interpretable and robust verification, prioritizing explicit numerical matching over weaker heuristics. The authors evaluate their system on the SciTab benchmark and, more importantly, through a series of rigorous stress tests designed to probe for \"true\" unit understanding.\n\nQuality: Exceptional\nThe paper is of very high quality. The methodology, while rule-based and thus less fashionable than large neural models, is technically sound, well-motivated, and perfectly suited for the problem it aims to solve: ensuring numerical reliability and interpretability. The core claims are strongly supported by a comprehensive evaluation. The ablation study is well-designed and provides clear evidence for the contribution of each system component, validating the priority-based architecture. The authors are commendably transparent about the system's performance relative to SOTA models and are upfront about the limitations of their approach. This honesty significantly strengthens the paper's credibility.\n\nClarity: Excellent\nThe paper is exceptionally well-written, organized, and easy to follow. The abstract and introduction clearly articulate the problem, the proposed solution, and the key contributions. The methods section describes the system with sufficient detail to understand its inner workings. The results are presented logically and effectively, with tables and analyses that are easy to interpret. The prose is clear, concise, and professional.\n\nSignificance: High\nThis work carries significant impact. In an era where the dominant paradigm is scaling neural networks, this paper provides a powerful and empirically-grounded reminder of the value of principled, symbolic systems for high-stakes domains where correctness and interpretability are paramount. The main contribution is not just the UnitMath system itself, but the rigorous evaluation methodology. The proposed stress tests for unit rescaling, percentage-type sensitivity, and dimensional consistency are a major contribution and should serve as a model for how to evaluate numerical reasoning capabilities in any system. The work convincingly argues that UnitMath is not necessarily a replacement for LLMs, but a vital complement, capable of systematically preventing a class of errors that current SOTA models are prone to. This will likely inspire future work in hybrid neuro-symbolic systems and more rigorous evaluation protocols for scientific AI.\n\nOriginality: High\nWhile rule-based systems are not new, the application of this specific, carefully designed architecture to the problem of unit-aware scientific fact verification is novel and timely. The paper's originality shines in its holistic approach: it combines a well-designed system with a bespoke, comprehensive evaluation framework that moves beyond simple accuracy metrics to test for genuine understanding. The focus on creating structured, interpretable reasoning traces as a primary output is another key point of novelty that sets it apart from opaque, end-to-end models.\n\nReproducibility: High\nThe authors describe their method in clear detail. For a deterministic, rule-based system, the provided description is likely sufficient for an expert to reimplement the core logic. Furthermore, the authors state in the checklist that code and data are provided with the submission, which would ensure full reproducibility.\n\nEthics and Limitations: Excellent\nThe authors have done an exemplary job of addressing the limitations and ethical implications of their work. The dedicated \"Limitations\" section is candid and thorough, discussing the inherent constraints of a rule-based architecture. The \"Broader Impacts\" and \"Code of Ethics\" sections are thoughtful, considering potential negative consequences such as over-reliance and the English-centric design, while responsibly positioning the system as an assistive tool rather than an autonomous decision-maker.\n\nSummary and Recommendation\nThis is an outstanding paper that makes a clear, strong, and important contribution. It tackles a critical weakness in modern AI systems with a rigorous and interpretable approach. The evaluation is a model of scientific thoroughness, and the results convincingly demonstrate the value of the proposed method. While UnitMath does not outperform the largest proprietary models on the headline metric for the SciTab dataset, it was not designed to; it was designed to be correct, reliable, and interpretable where those models are not. The paper successfully proves its point with overwhelming evidence from the stress tests. This is a complete, high-impact, and exceptionally well-executed piece of research that I believe will be of great interest to the community. It deserves a prominent place at the conference."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission122/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775930434,
    "mdate": 1760632171281,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission122/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "019SxqDtiK",
    "forum": "OG8sFxeNHv",
    "replyto": "OG8sFxeNHv",
    "content": {
      "decision": {
        "value": "Accept"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! Congratualations on the acceptance! Please see the reviews below for feedback."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission122/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948914463,
    "mdate": 1760632273913,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]