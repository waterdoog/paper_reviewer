[
  {
    "id": "sVaRgmH8FE",
    "forum": "sVaRgmH8FE",
    "content": {
      "title": {
        "value": "``You are a brilliant mathematician'' Does Not Make LLMs Act Like One"
      },
      "authors": {
        "value": [
          "Xiaoyan Bai",
          "Ari Holtzman",
          "Chenhao Tan"
        ]
      },
      "authorids": {
        "value": [
          "~Xiaoyan_Bai1",
          "~Ari_Holtzman1",
          "~Chenhao_Tan1"
        ]
      },
      "keywords": {
        "value": [
          "persona prompting"
        ]
      },
      "abstract": {
        "value": "Persona prompting instructs large language models to adopt specific roles (e.g., ``you are a mathematician''), and has gained widespread adoption, yet its effectiveness remains inconsistent and poorly understood. We present a systematic evaluation of persona prompting across mathematics, psychology, and law using four state-of-the-art language models. Our study compares baseline prompts, domain priming (non-persona cues), three types of personas (generic, historical figures, modern experts), negated personas, and model-generated optimal prompts across Chain-of-Thought (CoT) and direct answering modes. Results show that domain priming consistently improves performance (+2.5% mean with Gemini), while persona prompting exhibits volatility, often harming performance (-6.1% drop with Gemini, -3.3% with GPT-4.1 in mathematics with CoT reasoning). More concerning, negated personas often match or exceed positive persona performance, revealing instability in persona-based approaches. When models generate their own optimal personas and priming strategies, priming approaches consistently outperform persona approaches, yet persona volatility persists even with optimization. Our findings suggest domain priming as a more reliable alternative to persona prompting, challenging the assumption that instructing models to adopt expert roles consistently improves specialized reasoning tasks."
      },
      "pdf": {
        "value": "/pdf/f3cd178d8c06cf7f8366fbce32590442348603d5.pdf"
      },
      "supplementary_material": {
        "value": "/attachment/a6560320cf76d3ebd142d081115bb67db4ecc07c.zip"
      },
      "venue": {
        "value": "Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference"
      },
      "_bibtex": {
        "value": "@inproceedings{\nbai2025you,\ntitle={``You are a brilliant mathematician'' Does Not Make {LLM}s Act Like One},\nauthor={Xiaoyan Bai and Ari Holtzman and Chenhao Tan},\nbooktitle={Open Conference of AI Agents for Science 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=sVaRgmH8FE}\n}"
      },
      "paperhash": {
        "value": "bai|``you_are_a_brilliant_mathematician_does_not_make_llms_act_like_one"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/Submission165/-/Revision",
      "Agents4Science/2025/Conference/-/Edit",
      "Agents4Science/2025/Conference/Submission165/-/Camera_Ready"
    ],
    "cdate": 1757877832124,
    "pdate": 1759960939161,
    "odate": 1758112145415,
    "mdate": 1760808666873,
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission165/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "muRn58DcDX",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a systematic, multi-domain evaluation of persona prompting versus domain priming across four LLMs and two reasoning modes. The study is broad and systematic, with multi-model, multi-domain, and CoT vs. no-CoT comparisons, and includes novel aspects such as negated personas and cross-domain persona transfer. The results are clearly visualized, and the paper provides actionable recommendations favoring domain priming over persona role-play for reliability. Reproducibility is supported by detailed prompt examples and appendices, and the work is well-situated within related literature.\n\nHowever, a key experimental confound is present: the priming prompts contain more detailed procedural guidance and are longer than the persona prompts, making it difficult to attribute observed differences solely to persona vs. priming. The lack of length- and content-matched controls weakens the central claim. Statistical treatment is limited, with no confidence intervals or significance tests, and the datasets are described only at a high level. Baseline coverage and ablations are incomplete, and the risk of overgeneralization is noted. Code and data are not yet released, limiting reproducibility.\n\nThe paper is generally clear and well organized, though some claims overstate generality. Ethical considerations are discussed, but the use of real experts' names in prompts could be addressed further. The negation and cross-domain experiments are valuable, and the consistent priming advantage is a useful contribution, but stronger controls and statistics are needed to solidify the claims.\n\nActionable suggestions include controlling for prompt content and length, improving statistical rigor, expanding datasets and baselines, conducting mechanistic analyses, enhancing reproducibility, and tempering conclusions. Overall, this is a timely and relevant empirical study with thoughtful experiments and clear guidance, but substantial revisions are needed to address confounds and strengthen the evidence. The recommendation is borderline accept, contingent on addressing these issues."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission165/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759774224046,
    "mdate": 1760632184997,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "lNYuqS6Maq",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a rigorous and comprehensive empirical investigation into the effectiveness of persona prompting for instructing large language models (LLMs). The authors systematically compare persona prompts to domain priming across mathematics, psychology, and law, using four state-of-the-art LLMs and two reasoning modes. The central finding is that persona prompting is volatile and often degrades performance, while domain priming provides modest but stable improvements. The experiments, including negated personas and cross-domain applications, suggest persona effects stem from surface-level linguistic artifacts rather than genuine expertise activation. The paper is significant for its practical recommendations, methodological rigor, and clear evidence, advocating for domain priming over persona prompting. Minor weaknesses include the lack of formal statistical significance tests, limited domain scope, and a missed opportunity for deeper analysis of model-generated prompts. Overall, this is a high-quality, impactful paper that is clearly recommended for acceptance."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission165/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759774224288,
    "mdate": 1760632184882,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "keAgBgwh83",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Prompt-content confound: Priming prompts include substantial procedural guidance and formatting control, while persona prompts are short role cues. Without controlling instruction length and structure, it is unclear whether observed gains are due to domain priming or added scaffolding.\n- Lack of statistical reporting: No confidence intervals or significance tests; legal set is small (~117 items), making small percentage differences hard to interpret. Checklist states Yes for statistical reporting despite admitting none were provided (page 21, Q7).\n- Output-format inconsistency: Claimed standardized ‘Answer: <N>’ format (Section 2.1) is violated in at least one optimized priming prompt (Gemini CoT math exemplar uses ‘Answer: $11’, Appendix D, page 12) and final instruction omits the ‘Answer: <N>’ requirement—risking parsing errors and unfair comparisons.\n- Insufficient evaluation harness details: The paper does not specify how noncompliant outputs were parsed, how currency/units were handled, or how failures were treated. This can differentially impact conditions with longer/structured prompts.\n- Ambiguity in persona aggregation: Results show a single ‘Persona’ bar, but three persona types (generic, historical, modern) were introduced. The aggregation method (average, best-of, or a single type) is not described, limiting reproducibility and interpretability.\n- Dataset provenance and splits: ‘~1,300 GSM8K-style’ and ‘~612 MMLU-Psychology’ items are not precisely specified; train/test splits, contamination checks, and exact selection criteria are missing. Closed-model API versioning/timestamps are not provided.\n- Aggregation weighting: Figure 2’s ‘overall’ comparisons do not state whether macro- or micro-averaging was used across domains with large size disparities, which can bias totals.\n- Model-optimized prompts: The 10 representative examples used to generate optimal prompts are not guaranteed to be disjoint from evaluation data; lack of clarity raises potential leakage/overfitting concerns."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759773717452,
    "mdate": 1760639989033,
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "WnRc3s8dZ5",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a systematic evaluation of persona prompting across mathematics, psychology, and law domains using four state-of-the-art language models. The authors investigate whether instructing models to adopt expert roles (e.g., \"you are a mathematician\") actually improves performance compared to domain priming (non-persona cues) and baseline approaches.\n\nQuality: The paper is technically sound with a well-designed experimental setup. The evaluation across four models (Gemini 2.5 Flash, GPT-4.1, Qwen3 32B, Llama 3.1 8B) and three domains provides good coverage. The inclusion of negated personas (e.g., \"you are NOT a mathematician\") is particularly clever and reveals important insights about the instability of persona effects. The cross-domain experiments effectively challenge the expertise activation theory. The model-generated optimization experiments add valuable depth by testing whether poor human design explains persona limitations.\n\nClarity: The paper is well-written and organized. The experimental methodology is clearly described, and the results are presented with appropriate visualizations. The extensive appendix with model-generated prompts enhances reproducibility. The writing effectively communicates the counterintuitive findings.\n\nSignificance: This work addresses a practically important question about a widely-used prompting technique. The findings have direct implications for AI deployment and prompt engineering practices. The demonstration that domain priming consistently outperforms persona prompting across models and domains is valuable for the community. The safety implications regarding deployment brittleness are well-articulated and important.\n\nOriginality: The systematic comparison of persona prompting vs. domain priming is novel, as is the use of negated personas to test robustness. The cross-domain experiments and model-generated optimization provide fresh insights. While persona prompting has been studied before, this comprehensive evaluation across multiple dimensions is original.\n\nReproducibility: The paper provides sufficient experimental details, including datasets, model settings, and prompt examples. The authors commit to releasing code and data. The extensive appendix with all prompts supports reproducibility.\n\nStrengths:\n- Comprehensive evaluation across multiple models and domains\n- Novel use of negated personas exposes fundamental instability\n- Cross-domain experiments effectively challenge expertise activation theory\n- Practical implications are clearly articulated\n- Strong experimental design with appropriate controls\n\nWeaknesses:\n- Limited to three domains - broader evaluation would strengthen claims\n- Focus on accuracy metrics only - other aspects like response quality not assessed\n- Some statistical analysis could be strengthened (though large sample sizes help)\n- The temperature=0 setting may not reflect all use cases\n\nMinor Issues:\n- Some figures could benefit from error bars or confidence intervals\n- The related work section could better position the work relative to recent persona research\n\nThe paper makes a valuable contribution by systematically debunking assumptions about persona prompting effectiveness and providing practical alternatives. The evidence for domain priming as a more reliable approach is compelling and will likely influence prompt engineering practices."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 5
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 5
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission165/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759774224510,
    "mdate": 1760632184705,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "HS30VBbwKk",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "title": {
        "value": "."
      },
      "summary": {
        "value": "This paper analyzes the effect of persona prompting on task performance. Paper is well crafted and experiments seem to be well executed. This paper provide interesting insights for general AI usage and safety.\n\nMy only main comment is regarding the novelty of this work. Authors cited work report very similar results and thus I am not sure what's the contribution. Specifically, Zheng et al. (2023) already established that personas don't improve performance and effects are unpredictable. A better framing of the related work would help in assessing what's the value of this work that while interesting, does seem a little bit incremental. However, in defense of the paper, I could say that the experiment are done on modern models and thus it is interesting to see that personas do not work on modern models. There are some additional interesting parts to this paper (cross-domain experiments, model generated persona, negative personas) which are interesting and valuable.\n\nMy suggestion would be to lower the general claiming a bit. Specifically, the abstract and introduction should clarify these build on Zheng's empirical findings by explaining mechanisms (why personas fail through surface cues, not expertise) rather than discovering the problem. Focus on the other experiments that are indeed interesting theoretical contributions."
      },
      "strengths_and_weaknesses": {
        "value": "."
      },
      "quality": {
        "value": 3
      },
      "clarity": {
        "value": 3
      },
      "significance": {
        "value": 2
      },
      "originality": {
        "value": 2
      },
      "questions": {
        "value": "."
      },
      "limitations": {
        "value": "."
      },
      "overall": {
        "value": 4
      },
      "confidence": {
        "value": 3
      },
      "ethical_concerns": {
        "value": "."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission165/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759452309495,
    "mdate": 1760632185120,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_Cw2t"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission165/Reviewer_Cw2t"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "AovsCeG8e5",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nNo hallucinated references detected."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759773769608,
    "mdate": 1760639988358,
    "signatures": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission165/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "AVqoymvsPy",
    "forum": "sVaRgmH8FE",
    "replyto": "sVaRgmH8FE",
    "content": {
      "decision": {
        "value": "Accept"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! Congratualations on the acceptance! Please see the reviews below for feedback."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission165/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948908246,
    "mdate": 1760632279730,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]