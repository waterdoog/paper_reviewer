[
  {
    "id": "zsted1ZqII",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces Self-Spec, a deterministic, prompt-only orchestration for LLM code generation where the model designs its own compact specification schema, instantiates a task-level spec, resolves ambiguities via a minimal Q&A loop, obtains explicit confirmation, and then implements code strictly from the agreed spec. On HumanEval with pass@1 and T=0, Self-Spec improves GPT-4o from 87% to 92% (+5) and Claude 3.7 from 92% to 94% (+2), while Claude 3.5 dips from 90% to 89% (−1), with analysis attributing the dip to over-defensive guards. The approach is model-agnostic, uses no finetuning, and releases prompts and code. The orchestration is simple, coherent, and technically sound, with deterministic evaluation and thoughtful error analysis. However, the central scientific claim about the value of self-authored spec schemas is not isolated by ablation, and comparisons to strong structured prompting baselines are missing. The paper is clearly written and reproducible, but its impact is limited by narrow scope (HumanEval only), lack of broader benchmarks, and no open-source model evaluations. The novelty lies in the model-authored spec schema, but the lack of direct comparison to human-designed schemas undermines the originality claim. The paper is ethical and covers related work well, though it misses some recent structured prompting frameworks. Major strengths include clean orchestration, reproducible evaluation, and actionable error analysis. Major weaknesses are missing ablations, limited external validity, no cost/latency analysis, and potential inflation of performance due to simulated Q&A. Actionable feedback includes adding ablations, expanding evaluation to more benchmarks and models, providing cost analysis, statistical significance testing, clarifying Q&A constraints, analyzing schema qualities, and including a user study. Verdict: well-written and reproducible with a clean idea and small but meaningful gains, but not enough experimental evidence to substantiate the central claim. Overall recommendation: Borderline accept."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission206/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775308267,
    "mdate": 1760632196600,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "w5xgIyywUQ",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "decision": {
        "value": "Accept"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! Congratualations on the acceptance! Please see the reviews below for feedback."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission206/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948902203,
    "mdate": 1760632283906,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "em8CvqDwQv",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Statistical reporting: Table 1 (page 6) provides only point estimates; no confidence intervals, no significance tests (e.g., McNemar on paired outcomes), and no per-task breakdown. The checklist’s assertion that T=0 ‘covers’ statistical significance is incorrect.\n- Potential confound from simulated user: The FMInterviewer’s questions are answered by a second LLM (Section 3; Appendix A.3.7), which may introduce defaults/assumptions beyond the original docstrings. The native baseline cannot ask clarifying questions, raising fairness concerns.\n- Ambiguity about SpecDesigner scope: ‘One-time’ schema creation (Section 3) is not explicit about whether the GlobalSPEC is fixed across all 164 tasks per model/session; session boundaries and reuse policy are not clearly specified and could affect reproducibility.\n- Prompt preprocessing: HumanEval items are converted to ‘concise natural-language problem statements’ (Section 4). While both arms share this input (fair internally), this deviates from canonical setups; details of the transformation are not fully described in the main text.\n- Post-hoc modification claim: The statement that Claude 3.5 returns to baseline after removing over-defensive guards is not quantified in Table 1 and involves altering generated code after the fact, which is outside the defined evaluation protocol.\n- Closed-source model version drift: Results depend on GPT-4o and Claude 3.7/3.5; versioning and provider-side updates (Section 8) threaten exact reproducibility, despite logging attempts.\n- No ablations: The paper does not isolate contributions of individual roles (SpecDesigner/Interviewer/Confirmer) or test alternatives (e.g., spec without Q&A, Q&A without self-authored schema).\n- Limited scope: Only HumanEval (164 tasks) and Python; no evaluation on MBPP/APPS/SWE-bench or languages beyond Python, limiting generalizability.\n- Regression analysis limits: Table 3 cites self-spec failures but notes ‘baseline not provided,’ hindering precise attribution of regressions vs. shifts from baseline.\n- Technical detail gap: The identity/configuration of the second LLM used for simulated answers/confirmation is not specified in the main text (model choice can influence outcomes)."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776666335,
    "mdate": 1760640267463,
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "cmB0KCo0e7",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces SELF-SPEC, a novel and lightweight orchestration method for improving the reliability of LLM-based code generation. The core idea is to have the language model first design its own task-specific specification language (a \"spec\"), and then generate code strictly from an instance of that spec that has been populated and confirmed through a minimal Q&A loop. The authors hypothesize that a model-authored specification aligns better with the model's internal representational biases, thereby reducing ambiguity and errors common in direct natural language-to-code generation.\n\nThe paper is exceptionally well-written, clearly motivated, and positions itself effectively within the existing literature. It makes a compelling case for a \"middle path\" between unstructured, free-form reasoning (like Chain-of-Thought) and rigid, often off-distribution formal intermediate representations (like Dafny).\n\n**Quality:** The technical quality of this work is very high. The proposed SELF-SPEC pipeline is logically sound, well-structured, and thoughtfully designed. The experimental setup is rigorous: it uses the standard HumanEval benchmark, state-of-the-art models (GPT-4o, Claude 3 series), a strong baseline (direct generation), and deterministic decoding (T=0) to ensure fair and reproducible comparisons. The reported results—a +5 point pass@1 improvement for GPT-4o and +2 for Claude 3.7—are substantial at this high level of performance and strongly support the central claims. The authors' honesty and thoroughness are commendable, particularly in their analysis of the slight performance dip for Claude 3.5, which they convincingly trace to \"over-defensive coding\" rather than a flaw in the core method. This level of detailed error analysis adds significant credibility to the work.\n\n**Clarity:** The paper is a model of clarity. The abstract and introduction perfectly frame the problem, the proposed solution, and the key results. The methodology is explained with precision, and Figure 1 provides an excellent visual summary of the orchestration process. The writing is concise, professional, and accessible. The inclusion of prompt templates and a detailed appendix further enhances the clarity of the proposed method.\n\n**Significance:** The work is highly significant. The performance improvements are impressive in their own right, but the conceptual contribution is even more impactful. The idea of a \"model-authored DSL\" is a profound shift in perspective from forcing models to conform to human-designed formalisms. This could inspire a new wave of research into eliciting and leveraging models' internal representations for more reliable and aligned agentic behavior. The practical applications, especially for non-expert programmers like scientists—a key audience for the Agents4Science conference—are substantial and well-articulated.\n\n**Originality:** The paper is highly original. To my knowledge, this is the first systematic study of letting an LLM design its own specification language for code generation and then strictly adhering to it. While related concepts like intermediate reasoning and formal methods exist, SELF-SPEC carves out a novel and compelling niche. The authors do an excellent job of differentiating their work from prior art, clearly identifying the gap their contribution fills.\n\n**Reproducibility:** The authors have made an exemplary effort to ensure reproducibility. They provide an anonymous link to their code, prompts, and experimental setup. They specify the exact model versions and parameters used. The use of deterministic decoding is a key choice that facilitates verification of their results. This meets the highest standards of reproducibility.\n\n**Ethics and Limitations:** The authors provide a dedicated and thoughtful discussion of limitations and future work. They acknowledge the scope of their evaluation and the challenges of model versioning. More importantly, their proposed future work directly addresses the failure modes observed in their analysis, demonstrating a clear path forward. There are no ethical concerns with the research.\n\n**Conclusion:**\nThis is an outstanding paper that presents a novel, elegant, and effective solution to a critical problem in AI-driven science and software development. It is technically flawless, empirically strong, and conceptually groundbreaking. The work is presented with exceptional clarity and a commitment to reproducibility. It is a perfect fit for the Agents4Science conference and is likely to have a significant and lasting impact on the field. I recommend it for acceptance without hesitation."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission206/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775308565,
    "mdate": 1760632196351,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "YjugyuAhCV",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents Self-Spec, a novel approach where large language models author their own specification languages before generating code. The method involves a 6-step orchestration process where the model designs a schema, instantiates it from natural language requirements, resolves ambiguities through Q&A, and only generates code after confirming the specification.\n\nQuality:\nThe paper is technically sound with a well-designed experimental approach. The core idea is compelling - having models create their own intermediate representations that align with their internal biases rather than forcing them into human-designed formal specifications. The evaluation on HumanEval using deterministic decoding (T=0) with three state-of-the-art models (GPT-4o, Claude 3.7, Claude 3.5) is appropriate. The results show meaningful improvements for stronger models (+5 for GPT-4o, +2 for Claude 3.7) with detailed error analysis explaining the slight regression for Claude 3.5. The authors provide honest assessment of limitations and failure modes.\n\nClarity:\nThe paper is well-written and clearly structured. The motivation is compelling, the method is explained with sufficient detail including helpful figures, and the results are presented transparently. The orchestration pipeline is described systematically with clear role definitions for each component. The appendix provides extensive implementation details including prompt templates and examples.\n\nSignificance:\nThis work addresses an important problem in LLM code generation - the reliability gap between direct natural language-to-code generation and formal specification approaches. The contribution is conceptually significant as it represents the first systematic study allowing LLMs to design their own specification languages. The practical implications for non-expert programmers (domain scientists) could be substantial. The approach offers a practical middle ground between brittle direct generation and off-distribution formal methods.\n\nOriginality:\nThe core contribution is novel - letting models author their own specification languages rather than imposing human-designed formal intermediate representations. While related work exists on formal specifications and intermediate reasoning, this specific approach of model-authored DSLs for code generation appears to be genuinely new. The comparison to existing formal methods (Dafny) and positioning relative to chain-of-thought reasoning is appropriate.\n\nReproducibility:\nExcellent reproducibility provisions. The authors provide code, prompts, model identifiers, experimental configurations, and evaluation harness details. All materials needed to reproduce Table 1 results are made available. The deterministic decoding approach enhances reproducibility.\n\nEthics and Limitations:\nThe authors provide a dedicated limitations section and discuss future work directions. They acknowledge scope limitations (single benchmark), model versioning issues, and provide specific technical improvements for addressing remaining failure modes. The broader impacts are discussed appropriately.\n\nCitations and Related Work:\nThe related work section is comprehensive, properly positioning the work relative to formal specifications, intermediate reasoning approaches, and spec-driven pipelines. Citations appear accurate and complete.\n\nMinor Issues:\nThe evaluation is limited to HumanEval, though this is acknowledged. The improvement margins, while meaningful, are relatively modest. Some analysis could benefit from comparison to other intermediate reasoning approaches beyond the baseline.\n\nOverall Assessment:\nThis is a solid contribution that introduces a genuinely novel and practical approach to improving LLM code generation reliability. The idea is conceptually interesting, the execution is competent, and the results demonstrate clear value. The work opens up new research directions in model-authored specifications and provides immediate practical benefits. While not groundbreaking, it represents meaningful progress on an important problem with good experimental validation."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission206/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775308806,
    "mdate": 1760632196212,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "QvVkZVlDAN",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nPlease look at your references to confirm they are good.\n\n**Examples of references that could not be verified (they might exist but the automated verification failed):**\n\n- Hello GPT-4o by OpenAI\n- Alloy tools by Alloy Team"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777825260,
    "mdate": 1760640266741,
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "Onztd1Epb4",
    "forum": "6pr7BUGkLp",
    "replyto": "6pr7BUGkLp",
    "content": {
      "title": {
        "value": "Review of Self-Spec for reliable LLM code gen"
      },
      "summary": {
        "value": "This paper introduces Self-Spec, a prompting-based pipeline where an LLM generates its own specification schema for a coding problem, instantiates it for the given task, and optionally resolves ambiguities with a lightweight Q&A loop before generating code from the spec. The intuition is that self-authored specs align better with the model’s internal representations, reducing docstring drift and common logic errors. Evaluations on HumanEval with GPT-4o, Claude 3.7, and Claude 3.5 show consistent gains for the stronger models (GPT-4o: +5 pass@1, Claude 3.7: +2), though Claude 3.5 slightly regresses due to over-defensive guards."
      },
      "strengths_and_weaknesses": {
        "value": "Quality\n- Strengths: The idea of model-authored specifications is technically sound and well-motivated. Experiments on HumanEval are carefully executed with multiple LLMs (GPT-4o, Claude 3.7, Claude 3.5). The authors provide both quantitative results (pass@1 gains for stronger models) and qualitative error analysis (e.g., guard insertion failures). The method is practical, lightweight, and reproducible with shared prompts.\n- Weaknesses: Evaluation scope is narrow (only HumanEval, Python). Improvements are modest (+5 and +2 points), and one model regresses. The paper does not quantify inference overhead of the Q&A confirmation loop, nor does it compare empirically with alternative intermediate representations (e.g., CoT, self-consistency). Overall, this feels like a work-in-progress contribution — strong idea, but incomplete validation.\n\nClarity\n- Strengths: The paper is clearly written and logically structured. The pipeline is easy to follow, and the motivation for avoiding docstring drift is well explained. Reproducibility is supported by released prompts and logs.\n- Weaknesses: The positioning relative to formal IRs (Intermediate Representations) like Dafny/SMT constraints could be explained more explicitly for readers unfamiliar with program synthesis. The description of why Claude 3.5 regressed could be expanded with concrete examples. Overall, clarity is good but could benefit from more detailed baselines and framing.\n\nSignificance\n- Strengths: Opens an original line of inquiry — letting models define their own specs rather than relying on human-authored natural language or rigid IRs. This could influence future work on LLM-authored intermediates across domains (not just code).\n- Weaknesses: Current results are modest, and the scope is too limited to demonstrate broad community impact. Without experiments on larger benchmarks or non-code tasks, it is unclear whether the idea generalizes.\n\nOriginality\n- Strengths: The idea of LLM-authored specs is new and well-articulated as a middle ground between free-form natural language and formal IRs. The paper highlights this positioning clearly in related work.\n- Weaknesses: The originality claim would be stronger if comparisons were run against other lightweight intermediates (CoT, docstring standardization, schema prompts). Without these, it is harder to judge how unique the benefits are."
      },
      "quality": {
        "value": 2
      },
      "clarity": {
        "value": 3
      },
      "significance": {
        "value": 2
      },
      "originality": {
        "value": 2
      },
      "questions": {
        "value": "1. Benchmark scope: Could you extend evaluation beyond HumanEval to larger coding benchmarks (MBPP, APPS, SWE-Bench) or to other domains like SQL or math? This would strengthen both significance and quality.\n2. Overhead trade-offs: What is the runtime and cost overhead of the spec-generation + Q&A confirmation loop compared to direct prompting? Quantifying this would make the method more practical for adoption.\n3. Baseline comparisons: Could you add small-scale comparisons with chain-of-thought prompting or self-consistency to better situate Self-Spec against other lightweight intermediates?\n4. Regression analysis: For Claude 3.5, the model added defensive guards that degraded performance. Could you clarify with examples and suggest mitigation strategies (e.g., guard filtering, spec-level constraints)?\n5. Future integration: How might Self-Spec be extended toward more formal IRs or combined with lightweight validators? A forward-looking discussion would raise the significance."
      },
      "limitations": {
        "value": "Partially. The authors note model regressions and modest improvements, but limitations could be discussed more explicitly. In particular:\n- The narrow evaluation scope (HumanEval only).\n- The risk that model-authored specs could drift in unexpected ways or create fragile dependencies.\n- The potential overhead of adding confirmation loops.\n\nExpanding the discussion of these limitations, and clarifying that the method is exploratory rather than ready for deployment, would improve transparency."
      },
      "overall": {
        "value": 3
      },
      "confidence": {
        "value": 3
      },
      "ethical_concerns": {
        "value": "No major ethical concerns. The paper focuses on code generation benchmarks with open datasets and standard LLMs. No sensitive data or harmful applications are involved. No ethics review needed."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission206/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759392708187,
    "mdate": 1760632197036,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Reviewer_CAJ2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission206/Reviewer_CAJ2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "6pr7BUGkLp",
    "forum": "6pr7BUGkLp",
    "content": {
      "title": {
        "value": "Self-Spec: Model-Authored Specifications for Reliable LLM Code Generation"
      },
      "authors": {
        "value": [
          "Zihao Xu",
          "Xiao Cheng",
          "Jingling Xue",
          "Yuekang Li"
        ]
      },
      "authorids": {
        "value": [
          "~Zihao_Xu5",
          "~Xiao_Cheng2",
          "~Jingling_Xue1",
          "~Yuekang_Li1"
        ]
      },
      "keywords": {
        "value": [
          "Large Language Models",
          "NL-to-Code",
          "Specification-Driven Code Generation."
        ]
      },
      "abstract": {
        "value": "Do large language models (LLMs) code more reliably when they first author a task-specific specification language and then implement strictly from that spec? We introduce Self-Spec, a lightweight, deterministic (T=0) orchestration that prompts a model to (i) design a compact spec schema it prefers, (ii) instantiate that schema from a problem’s docstring and signature, (iii) resolve ambiguities via a minimal Q&A loop, and (iv) generate code only from the confirmed spec. The intuition is distributional: a self-authored spec better aligns with a model’s internal representational bias, reducing docstring drift and format/edge-case mistakes. On HumanEval (pass@1, single sample), Self-Spec improves over direct NL→code for stronger models: GPT-4o 87%→92% (+5) and Claude 3.7 92%→94% (+2); Claude 3.5 dips 90%→89% (-1), which returns to baseline once we remove over-defensive guards in generated code (e.g., replacing raise/assert with no-ops when unspecified). To our knowledge, this is the first systematic study that lets an LLM design its own spec language for coding. The method is simple (no finetuning), model-agnostic (each model chooses its spec shape), and practical (assumptions are made explicit). We release prompts and code for reproduction. Overall, our results show that Self-Spec works in practice and offers strong potential as a general path to more reliable LLM coding via self-authored specifications."
      },
      "pdf": {
        "value": "/pdf/a16111541a1eef1ac87cb603b29c867b932e4b93.pdf"
      },
      "supplementary_material": {
        "value": "/attachment/6a535b9bd537e9884b878be7a29fc23dae6fa196.zip"
      },
      "venue": {
        "value": "Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference"
      },
      "_bibtex": {
        "value": "@inproceedings{\nxu2025selfspec,\ntitle={Self-Spec: Model-Authored Specifications for Reliable {LLM} Code Generation},\nauthor={Zihao Xu and Xiao Cheng and Jingling Xue and Yuekang Li},\nbooktitle={Open Conference of AI Agents for Science 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=6pr7BUGkLp}\n}"
      },
      "paperhash": {
        "value": "xu|selfspec_modelauthored_specifications_for_reliable_llm_code_generation"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/Submission206/-/Revision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1757942434528,
    "pdate": 1759960941014,
    "odate": 1758112145415,
    "mdate": 1760776025346,
    "signatures": [
      "Agents4Science/2025/Conference/Submission206/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission206/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]