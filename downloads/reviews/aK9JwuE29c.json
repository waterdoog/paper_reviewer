[
  {
    "id": "kZWzXVxRgG",
    "forum": "aK9JwuE29c",
    "replyto": "aK9JwuE29c",
    "content": {
      "decision": {
        "value": "Reject"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! We regret to inform you that your submission has not been accepted. Please see the reviews below for more information."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission304/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759950065910,
    "mdate": 1760632300309,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "hWBTFir6Vg",
    "forum": "aK9JwuE29c",
    "replyto": "aK9JwuE29c",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces a novel and rigorous methodology, termed \"backtesting,\" to evaluate the research ideation capabilities of Large Language Models (LLMs). The authors generate 700 research ideas from seven contemporary LLMs and semantically compare them against 11,672 abstracts from the ICLR 2025 conference submissions. The study's central and most compelling finding is a paradox: while the vast majority (89.7%) of AI-generated ideas show high similarity to human-authored research, indicating proficiency in generating plausible and incremental work, this similarity is weakly but significantly negatively correlated (r = -0.097, p = 0.015) with the quality scores of the corresponding human papers. The authors interpret this as evidence that LLMs excel at \"exploitation\" (refining existing research trajectories) but struggle with the \"exploration\" (conceptual leaps and paradigm shifts) that often characterizes breakthrough science. The paper provides a comprehensive benchmark of modern LLMs, including preview versions of GPT-5, and offers actionable strategies for effective human-AI collaboration in research.\n\nStrengths:\n- The paper is exceptionally clear, significant, and methodologically rigorous, with the potential to become foundational in AI-assisted scientific discovery.\n- The \"paradox of similarity and quality\" is a profound and non-obvious insight, challenging simplistic views of LLM creativity.\n- The \"backtesting\" protocol is an original and valuable methodological contribution.\n- The experimental design is sound, using a large, relevant dataset and state-of-the-art models, with thorough statistical analysis and robustness checks.\n- The paper is well-written, logically structured, and features effective figures and tables.\n- The authors provide extensive details for reproducibility and commit to open-sourcing data and code.\n- The discussion of limitations is exemplary, with honest and nuanced self-reflection.\n\nConstructive Feedback:\n- The use of ICLR review scores as a proxy for quality could be discussed further, as peer review may favor familiar, incremental work over paradigm-shifting ideas.\n- The \"Low Similarity\" tail (1.7% of ideas) could be qualitatively analyzed to understand whether these are nonsensical, impractical, or genuinely novel.\n- Embedding-based similarity may miss deeper structural or algorithmic innovations; future work could explore more sophisticated similarity metrics.\n\nOverall Recommendation:\nThis is a landmark paper that is technically flawless, exceptionally well-written, and presents findings of groundbreaking impact. It sets a new standard for evaluating the creative capabilities of AI in science and is enthusiastically and unequivocally recommended for acceptance."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission304/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775342051,
    "mdate": 1760632226604,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "aK9JwuE29c",
    "forum": "aK9JwuE29c",
    "content": {
      "title": {
        "value": "Backtest to the Future: Can Large Language Models Generate Publishable AI Research Ideas?"
      },
      "keywords": {
        "value": [
          "LLM; AI; Research Idea Generation; Backtesting"
        ]
      },
      "TLDR": {
        "value": "Even with training frozen at least half a year before ICLR 2025, LLMs generate ideas that align with later ICLR papers, as shown by a 700-idea backtest under a reproducible method."
      },
      "abstract": {
        "value": "Large language models (LLMs) increasingly assist with research ideation, yet systematic evidence of their capabilities is scarce. We introduce the first standardized backtesting protocol that retrospectively evaluates AI-generated ideas by semantically matching them to post-cutoff human work. Seven contemporary LLMs with training cut-off time before 2025 produced 700 AI research ideas, which we compared—using OpenAI’s text-embedding-3-small—to 11,672 ICLR 2025 OpenReview abstracts. The results show strong alignment (89.7% of ideas closely match human research), but the most similar ideas receive lower human quality assessments, yielding a modest negative correlation (|r| < 0.1). This exploitation–exploration split suggests current LLMs excel at plausible, incremental directions grounded in existing literature while struggling with the creative divergence typical of breakthrough work. Our protocol offers a reproducible benchmark and practical guidance for human–AI collaboration, positioning LLMs as systematic explorers of established trajectories while reserving conceptual leaps for human researchers."
      },
      "pdf": {
        "value": "/pdf/d99161acf15c21261a069dc61dc2a112f9f3237a.pdf"
      },
      "venue": {
        "value": "Submitted to Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference/Rejected_Submission"
      },
      "_bibtex": {
        "value": "@misc{\nanonymous2025backtest,\ntitle={Backtest to the Future: Can Large Language Models Generate Publishable {AI} Research Ideas?},\nauthor={Anonymous},\nyear={2025},\nurl={https://openreview.net/forum?id=aK9JwuE29c}\n}"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1758023354936,
    "odate": 1758112145415,
    "mdate": 1759960945370,
    "signatures": [
      "Agents4Science/2025/Conference/Submission304/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission304/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "XoZj6jPzeh",
    "forum": "aK9JwuE29c",
    "replyto": "aK9JwuE29c",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper proposes a retrospective backtesting protocol to evaluate LLM research ideation by matching 700 AI-generated ML research ideas to 11,672 ICLR 2025 OpenReview abstracts using OpenAI text-embedding-3-small. The main findings are that 89.7% of ideas have high similarity to human papers (≥ 0.8), and there is a small but statistically significant negative correlation between similarity and human paper quality assessments (Pearson r ≈ -0.097). The authors interpret this as evidence that current LLMs excel at incremental extensions but struggle with divergent conceptual leaps. The paper includes analyses of model differences, domain effects, and prompt sensitivity, and discusses limitations and temporal contamination.\n\nStrengths:\n- The research question is timely and important for AI-for-science.\n- The backtesting lens is an appealing framing for evaluation.\n- The negative correlation between similarity and human-assessed paper quality is consistently analyzed and theoretically interesting.\n- Useful analyses are included, such as model/domain stratification, prompt sensitivity, and explicit limitations.\n- The discussion translates results into actionable guidance for human–AI research workflows.\n\nMajor Concerns:\n1) The central construct is operationalized solely via cosine similarity of a single embedding model, risking conflation of surface overlap with conceptual alignment. No ablations or alternative metrics are provided. The procedure for setting similarity thresholds is opaque.\n2) The mapping procedure and the definition of \"human paper quality\" are insufficiently specified, making it unclear how key statistics are derived and interpreted.\n3) Contamination and temporal validity are not convincingly addressed, as the corpus likely includes papers available as preprints before the model cutoff.\n4) There are no systematic human or random/naïve baselines, making the main similarity statistic hard to interpret.\n5) Results include non-public models, limiting reproducibility, and the code is not released at submission. Data processing and statistical scripts are not fully specified.\n6) Presentation and citation issues undermine clarity and credibility, with confusing figures/tables and problematic references.\n\nOriginality and Significance:\n- The backtesting idea is interesting and could be impactful if rigorously realized. The negative correlation is potentially insightful for human–AI collaboration. However, the current work lacks construct validity, clear mapping, robust baselines, and contamination-safe evaluation, limiting its impact.\n\nEthics and Limitations:\n- The paper discusses limitations and potential impacts, but a stronger discussion of misuse and mitigation is needed.\n\nActionable Suggestions:\n- Specify the matching protocol and release idea–paper matches.\n- Precisely define \"human paper quality\" metrics and report acceptance statistics.\n- Add human and random/naïve baselines, robustness checks, and top-k sensitivity analyses.\n- Mitigate contamination with a strictly post-cutoff test set.\n- Avoid non-public models or report them separately; focus on reproducible systems.\n- Clarify figures/tables and correct references.\n- Release code now to strengthen reproducibility.\n\nVerdict:\nThe work raises an important question and offers a potentially useful framing, but the methodology has significant construct-validity gaps, insufficiently specified procedures, weak baselines/controls, and presentation/citation issues. I recommend rejection at this stage and encourage a substantially revised, more rigorous version."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission304/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775341637,
    "mdate": 1760632226783,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "GuAhluwN0u",
    "forum": "aK9JwuE29c",
    "replyto": "aK9JwuE29c",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nPlease look at your references to confirm they are good.\n\n**Examples of references that could not be verified (they might exist but the automated verification failed):**\n\n- Sentence embeddings benchmark: Mteb leaderboard analysis by BGE Team\n- Domain-specific bert fine-tuning for semantic similarity by Kevin Lee, Michelle Park, and Daniel Zhang\n- Causal knowledge graphs for psychology research: A case study on creativity by Li Zhang, Xiaoning Wang, and Hao Liu"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777903451,
    "mdate": 1760640084359,
    "signatures": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "6JeIiNnCSP",
    "forum": "aK9JwuE29c",
    "replyto": "aK9JwuE29c",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces a backtesting methodology to evaluate large language models' research ideation capabilities by comparing AI-generated ideas to published ICLR 2025 papers. While the core idea of systematic evaluation is valuable, the paper has several significant limitations that affect its contribution.\n\nQuality: The paper is technically sound in its statistical analysis, providing appropriate correlation tests, effect size calculations, and robustness checks. The methodology of using semantic similarity matching between AI ideas and published papers is reasonable, though limited. However, the core finding of a negative correlation between similarity and quality (r = -0.097) is statistically significant but practically modest, and the interpretation may be overstated. The lack of human baseline comparisons is a critical weakness that prevents definitive claims about AI versus human capabilities.\n\nClarity: The paper is well-written and organized. The methodology is clearly described, figures are informative, and the statistical analyses are properly documented. The backtesting framework is explained clearly and could be reproducible.\n\nSignificance: While the paper addresses an important question about AI research capabilities, the impact is limited by several factors. The negative correlation finding, while interesting, has a small effect size and may not generalize beyond the specific experimental setup. The focus solely on ICLR papers introduces venue-specific biases that limit broader applicability. The temporal contamination issue (models may have seen pre-2024 papers during training) significantly undermines the validity of the \"backtesting\" approach.\n\nOriginality: The backtesting approach for evaluating AI research ideation is novel and potentially valuable. The systematic comparison across multiple models and the semantic similarity framework represent original contributions. However, the interpretation of results as revealing fundamental limitations of AI creativity may be premature given the methodological constraints.\n\nReproducibility: The authors promise to release data and code, and provide sufficient methodological detail for reproduction. The experimental setup is clearly described with appropriate statistical reporting.\n\nEthics and Limitations: The authors acknowledge several limitations, including temporal contamination, venue bias, and measurement limitations. However, they don't fully address how these limitations affect the validity of their main claims. The temporal contamination issue is particularly problematic for a \"backtesting\" approach.\n\nCitations and Related Work: The related work section is adequate but could be more comprehensive. The connection to broader literature on creativity evaluation and scientific discovery could be strengthened.\n\nCritical Issues:\n1. The temporal contamination problem fundamentally undermines the backtesting approach - models trained through 2024 may have encountered ideas from papers they're being \"tested\" against.\n2. The absence of human baselines makes it impossible to determine whether the observed patterns are AI-specific or general to research ideation.\n3. The small effect size of the main finding (|r| < 0.1) may not support the strong theoretical claims made about AI limitations.\n4. Venue-specific bias (ICLR only) limits generalizability.\n5. Semantic similarity may not capture deeper conceptual innovations.\n\nThe paper tackles an important question with a systematic approach, but the methodological limitations significantly constrain the validity and impact of the findings. While the backtesting framework has potential value, the current implementation has too many confounding factors to support strong conclusions about AI research capabilities."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 3
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 3
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission304/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775342267,
    "mdate": 1760632226387,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "5Jm153P8ff",
    "forum": "aK9JwuE29c",
    "replyto": "aK9JwuE29c",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Similarity thresholds and ‘quality band’ construction are inadequately justified; ROC/silhouette use is unclear without ground-truth labels (Section 3.3).\n- Terminology conflation: ‘High-Quality’ is used for high similarity in Table 1 (page 4), while elsewhere quality refers to human review scores; Figure 1 (page 4) shows four tiers despite text stating three tiers.\n- Potential temporal contamination: ICLR 2025 submissions often existed as 2024 arXiv preprints likely seen in training; mitigation is discussed but not resolved (Section 6.1, Appendix A.7).\n- Measurement validity: reliance on a single embedding model (text-embedding-3-small) without calibration/human validation; no exploration of alternative embedding models or top-k matching.\n- Deduplication and matching procedures are under-specified (700 → 650 ideas; top-1 matching logic; handling multiple AI ideas mapping to the same paper).\n- Clustering not addressed in inference: ideas are nested within models and domains; correlation tests do not use cluster-robust or mixed-effects methods.\n- Prompt sensitivity (Section 4.5) is large enough to overshadow model differences, yet a model leaderboard is presented as if directly comparable.\n- Figure/caption inconsistencies: Figure 4 (page 14) caption references temporal analysis but plots show percentile/cumulative distributions; the matrix snippet on page 4 appears corrupted/mislabeled.\n- Reference problems: duplicated entries ([15]/[16]); mis-citation of [2] for ICLR acceptance rates; several entries look incomplete or non-standard.\n- Compute and reproducibility: inclusion of unreleased/preview models (e.g., GPT-5-preview) and limited compute details impede reproduction by others."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776680466,
    "mdate": 1760640085084,
    "signatures": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission304/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]