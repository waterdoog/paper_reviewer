[
  {
    "id": "uE5bk2buaU",
    "forum": "Iu35dffCJZ",
    "replyto": "Iu35dffCJZ",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Risk measure inconsistency: Intro discusses mean-upper-semideviation ρ(Z) = E[Z] + c||(Z−E Z)+||_p, but Section 2.1 adopts ρp(JC) = (E[JC^p])^{1/p} (Eq. (1)), leading to contradictory properties and claims.\n- Incorrect claim that the Lp family includes CVaR as p → ∞; the Lp norm converges to the essential supremum, not CVaR.\n- Flawed Lemma 1 inequality: with λ_{t+1} = [λ_t + ν_tΔ_t]_+, for Δ_t > 0 one has λ_{t+1}Δ_t = λ_tΔ_t + ν_tΔ_t^2 ≥ λ_tΔ_t, not ≤ as stated.\n- Invalid duality-gap and maximization over λ: max_{λ≥0} L(θ_t, λ) is unbounded if ρ(θ_t) < β; claimed maximizer λ = max(0, ρ_t − β) is incorrect.\n- Unproven strong duality/convexity assumptions for the RL problem; mapping from policies to return distributions is non-convex, and no occupancy-measure reformulation is provided for E[JC^p].\n- Augmented MDP budget update ignores discounting (κ' = κ − c(s,a) instead of κ' = (κ − c(s,a))/γ), mis-specifying the discounted constraint enforcement.\n- Experimental metric mismatch: with Bernoulli cost C, ρp = (E[C])^{1/p}; reported risk values and constraint satisfaction contradict Eq. (1), indicating that E[C] (hazard probability) was plotted while claiming Mean-Lp risk.\n- Holding β fixed across p for Bernoulli cost changes the feasible set (E[C] ≤ β^p) but is not acknowledged, confounding comparisons across p.\n- Use of coherent-risk gradient estimators and CVaR analogies without applicability to the adopted non-coherent Lp norm risk.\n- No error bars/statistical tests; single small domain; claims about sample efficiency and constraint tightness rely on inconsistent risk computations."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776665145,
    "mdate": 1760640187409,
    "signatures": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "nq8FdIx5B1",
    "forum": "Iu35dffCJZ",
    "replyto": "Iu35dffCJZ",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "Summary:\nThe paper addresses reinforcement learning with an Lp-type risk constraint on cumulative cost, proposing two methods: a primal-dual policy gradient (PD-PG) method and a model-based dynamic programming method on an augmented state space. Experiments in a 5×5 gridworld are used to illustrate policy changes with p.\n\nStrengths:\n- Tackles general Lp risk constraints in RL, providing a spectrum between risk-neutral and worst-case behavior.\n- Proposes two complementary methods: a scalable model-free approach and a safety-enforcing baseline for small/tabular settings.\n- High-level motivation and algorithmic sketches are clear.\n- Code and a simple, reproducible environment are provided.\n- Qualitative illustration of how increasing p affects return and risk.\n\nMajor Concerns:\n1) Inconsistency and confusion about the risk measure: The paper inconsistently defines the risk measure, conflates different forms (mean-upper-semi-deviation vs. Lp norm), and makes incorrect claims about the relationship to CVaR.\n2) Theoretical claims are insufficiently justified and sometimes incorrect: Convergence claims are not rigorously established, and key technical challenges (nonconvexity, non-Lipschitz mappings, estimator variance) are not addressed in the analysis.\n3) Augmented MDP formulation has technical flaws: The budget update does not account for discounting, so the constraint is not correctly enforced in the discounted setting. Near-optimality claims are not rigorously derived.\n4) Experimental inconsistencies and weak evaluation: Contradictory claims about constraint satisfaction, trivialization of tail sensitivity in the gridworld setup, lack of error bars, limited domains, and missing baselines/ablations.\n5) Related work positioning and overclaiming: Overstates novelty and omits relevant literature; some citations are placeholders without demonstrated applicability.\n\nClarity and Reproducibility:\n- High-level presentation is readable, but inconsistent definitions and incorrect claims reduce clarity.\n- Missing algorithmic details needed for reproduction.\n- Code is provided, but descriptions are insufficient for standalone reproducibility.\n\nEthics and Limitations:\n- No major ethical concerns. Some limitations are acknowledged, but central technical limitations are not discussed.\n\nActionable Suggestions:\n- Unify and precisely define the risk measure; correct statements about coherence and CVaR.\n- Provide rigorous analysis for PD-PG under clear assumptions, addressing estimator bias/variance and non-Lipschitz mappings.\n- Fix the augmented MDP for discounting and provide rigorous near-optimality bounds.\n- Strengthen experiments: add error bars, baselines, more diverse environments, and ablations; ensure constraints are respected.\n- Correct inaccurate claims and clarify relationships to related work.\n\nOverall Assessment:\nThe paper addresses an important problem with promising ideas, but suffers from fundamental technical issues (inconsistent risk definition, incorrect claims, flawed augmented MDP), overclaims in theory, and weak/contradictory empirical evaluation. In its current state, it does not meet the bar for acceptance."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission16/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775305072,
    "mdate": 1760632146895,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "mL2qgpUJ3D",
    "forum": "Iu35dffCJZ",
    "replyto": "Iu35dffCJZ",
    "content": {
      "decision": {
        "value": "Reject"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! We regret to inform you that your submission has not been accepted. Please see the reviews below for more information."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission16/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759950678609,
    "mdate": 1760632262034,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "jrGIZMk90F",
    "forum": "Iu35dffCJZ",
    "replyto": "Iu35dffCJZ",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a comprehensive framework for reinforcement learning under mean-Lp risk constraints, a general and flexible family of risk measures that can interpolate between risk-neutral (expected cost) and worst-case (max cost) objectives. The authors propose two complementary algorithmic solutions: a model-free, primal-dual policy gradient (PD-PG) method and a model-based, augmented MDP dynamic programming (Aug-MDP) approach. The paper is exceptionally well-written, technically rigorous, and addresses a significant and timely problem in safe and reliable AI.\n\nQuality: The technical quality of this submission is outstanding. The problem formulation is clear and well-motivated, highlighting the challenges of optimizing the non-Lipschitz Lp-risk measure. The two proposed algorithms are principled and well-suited to the problem. The theoretical analysis is the main strength of the paper. For the PD-PG algorithm, the authors prove convergence to an epsilon-optimal and epsilon-feasible policy with a sample complexity of O(1/epsilon^2). This is a very strong result, as it matches the best-known rates for much simpler constrained MDPs with linear constraints, demonstrating that the complexity of the nonlinear Lp-risk does not degrade the theoretical convergence rate. The analysis for the Aug-MDP method is also sound, correctly establishing its optimality for the stricter p=infinity constraint and providing a solid argument for its near-optimality for the original problem with large p. The experimental evaluation, while conducted in a simple gridworld, is effective and well-designed. It clearly illustrates the core contribution: the ability to control the agent's risk aversion by tuning 'p' and successfully learn policies that satisfy the given risk constraint.\n\nClarity: The paper is a model of clarity. The abstract and introduction perfectly frame the problem, the challenges, and the paper's contributions. The mathematical notation is precise and consistent. The structure of the paper is logical, guiding the reader from the general problem formulation to the specific details of each proposed algorithm and their theoretical properties, followed by empirical validation. The inclusion of algorithm pseudocode and detailed explanations makes the proposed methods easy to understand. The appendix provides comprehensive proofs and further discussion, which is exemplary.\n\nSignificance: The significance of this work is high. Risk-sensitive and safe RL are critical areas for deploying AI in real-world, safety-critical domains. Most prior work has focused on a few specific risk measures like CVaR or variance. This paper significantly broadens the scope of tractable risk-constrained RL by providing the first general-purpose, provably convergent algorithms for the entire family of Lp-risk measures. This gives practitioners a powerful and flexible tool to specify and control risk attitudes in a nuanced way. The theoretical results are of high importance to the RL theory community, and the proposed algorithms are practical enough to be adopted by applied researchers. This work is likely to inspire follow-up research on other nonlinear risk measures and more complex constrained optimization problems in RL.\n\nOriginality: The paper is highly original. While the constituent components (Lagrangian methods, state augmentation) are known concepts, their application, synthesis, and rigorous analysis in the context of Lp-risk-constrained RL are novel and non-trivial. The paper fills a clear and important gap in the literature by moving beyond the standard set of risk measures and providing strong, global convergence guarantees where prior work on nonlinear constraints often could not.\n\nReproducibility: The paper provides excellent support for reproducibility. The experimental setup is described in sufficient detail. Crucially, the authors have provided a link to the source code, which is the gold standard. The simplicity of the gridworld environment also aids in making the results easy to verify. The only minor point is the lack of error bars in the plots, although the text states that results are averaged over 20 runs, which provides some confidence in their stability.\n\nEthics and Limitations: The authors have done an excellent job of discussing the limitations of their work in a dedicated section in the appendix. They are transparent about the reliance on convexity assumptions for their theoretical guarantees, the potential for over-conservatism at high values of p, and the sample-efficiency challenges inherent in learning about rare, high-risk events. This level of honest self-assessment is commendable. There are no ethical concerns; on the contrary, the work contributes positively towards the development of safer and more reliable AI agents.\n\nConclusion: This is an outstanding paper that makes a foundational contribution to the field of risk-constrained reinforcement learning. It is technically flawless, clearly presented, and addresses a problem of high significance. It provides a novel, general framework and backs it up with rigorous theory and clear empirical validation. This work sets a new standard for research in this area and is a clear \"Strong Accept\" for the Agents4Science conference."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission16/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775305334,
    "mdate": 1760632146738,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "Iu35dffCJZ",
    "forum": "Iu35dffCJZ",
    "content": {
      "title": {
        "value": "Mean-Lp Risk-Constrained Reinforcement Learning: Primal-Dual Policy Gradient and Augmented MDP Approaches"
      },
      "keywords": {
        "value": [
          "Constrained MDP (CMDP)",
          "risk measures",
          "safe RL"
        ]
      },
      "abstract": {
        "value": "Convex risk measures allow decision-makers to account for uncertainty beyond standard expectations, and have become essential in safety-critical domains. One widely used example is the Conditional Value-at-Risk (CVaR), a coherent risk metric that targets tail outcomes. In this paper, we consider a more general \nfamily of risk measures, the mean-$L^p$ risk for $p\\ge 1$, defined as the $L^p$-norm of a cost distribution; this family includes CVaR as an extreme case (as $p \\to \\infty$). We formulate a reinforcement learning problem in which an agent seeks to maximize reward subject to a mean-$L^p$ risk constraint on its \ncumulative cost. This problem is challenging due to the nested, non-Lipschitz structure of the $L^p$ risk measure, which hinders the use of standard policy optimization or dynamic programming techniques. To address this, we propose two complementary solution approaches: (1) a $\\textbf{primal-dual policy gradient \nalgorithm}$ that relaxes the risk constraint via a Lagrange multiplier, and (2) a $\\textbf{model-based dynamic programming method}$ that enforces the constraint by augmenting the state space with a cost budget. We prove that the policy-gradient approach converges to an $\\epsilon$-optimal safe policy with \n$\\tilde{O}(1/\\epsilon^2)$ samples, matching the best-known rate for simpler (risk-neutral or linear-constraint) cases. Meanwhile, the augmented MDP method computes a policy that never violates the cost limit and is nearly optimal for large $p$. Our results provide the first general-purpose algorithms for $L^p$-risk-constrained RL, generalizing prior approaches that were limited to \nCVaR or variance-based risk. We validate our theoretical results \nthrough experiments in a gridworld environment, demonstrating that both algorithms successfully learn policies that respect the risk constraint and adjust conservativeness as the risk sensitivity parameter $p$ varies."
      },
      "pdf": {
        "value": "/pdf/dc830dde76a43cf5871dc7e7cc36e4affd53de58.pdf"
      },
      "venue": {
        "value": "Submitted to Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference/Rejected_Submission"
      },
      "_bibtex": {
        "value": "@misc{\nanonymous2025meanlp,\ntitle={Mean-Lp Risk-Constrained Reinforcement Learning: Primal-Dual Policy Gradient and Augmented {MDP} Approaches},\nauthor={Anonymous},\nyear={2025},\nurl={https://openreview.net/forum?id=Iu35dffCJZ}\n}"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1754346662284,
    "odate": 1758112145415,
    "mdate": 1759960932668,
    "signatures": [
      "Agents4Science/2025/Conference/Submission16/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission16/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "BeuLFV1Jz0",
    "forum": "Iu35dffCJZ",
    "replyto": "Iu35dffCJZ",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper introduces a reinforcement learning framework for handling mean-Lp risk constraints (p ≥ 1), proposing two algorithms: a primal-dual policy gradient method and an augmented MDP approach. The theoretical development is sound and the generalization to Lp risk measures is novel, but the work relies on strong assumptions (convexity, exact gradients) that may not hold in practice. The augmented MDP approach is limited to small, tabular state spaces and known models. Experimental validation is severely limited to a simple 5×5 gridworld, with no evaluation on standard RL benchmarks or continuous control tasks, raising concerns about scalability and practical impact. The paper is well-written and clear, but the contribution feels incremental and the lack of substantial experiments is a major weakness. The authors acknowledge difficulties in scaling their methods, further limiting the work's significance. Overall, the paper addresses an interesting theoretical problem but falls short in practical validation and significance for a top venue."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 3
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 3
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission16/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775305557,
    "mdate": 1760632146579,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "AIpn68DOHd",
    "forum": "Iu35dffCJZ",
    "replyto": "Iu35dffCJZ",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nPlease look at your references to confirm they are good.\n\n**Examples of references that could not be verified (they might exist but the automated verification failed):**\n\n- Risk-Concerned Reinforcement Learning with Distributional Risk Measures by Yinlam Chow, Mohammad Ghavamzadeh, Aviv Tamar, Shie Mannor"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777754386,
    "mdate": 1760640186646,
    "signatures": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission16/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]