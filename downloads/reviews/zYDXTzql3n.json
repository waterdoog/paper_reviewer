[
  {
    "id": "zYDXTzql3n",
    "forum": "zYDXTzql3n",
    "content": {
      "title": {
        "value": "Multi-LLM and Multi-Prompt Strategies for COVID-19 Infodemic Detection in Chinese Social Media: An Empirical Evaluation"
      },
      "authors": {
        "value": [
          "Teng Zuo",
          "Hongwen Lin",
          "Lingfeng He",
          "Hongji Zeng",
          "Lina Tang",
          "Li He",
          "Ning Li"
        ]
      },
      "authorids": {
        "value": [
          "~Teng_Zuo1",
          "~Hongwen_Lin1",
          "~Lingfeng_He2",
          "zhengzhouzhj@qq.com",
          "leana@ruc.edu.cn",
          "lihegeo@xjtu.edu.cn",
          "ningli@cmu.edu.cn"
        ]
      },
      "keywords": {
        "value": [
          "Large Language Models",
          "Infodemic",
          "Social Media"
        ]
      },
      "TLDR": {
        "value": "Under optimal configuration, LLMs can assist in detecting COVID-19 misinformation on Chinese social media."
      },
      "abstract": {
        "value": "\\textbf{Objective:} Misinformation during the COVID-19 infodemic poses a serious public health risk. We investigate whether large language models (LLMs) can automatically identify COVID-19 misinformation in Chinese social media content, and how different prompting strategies affect performance. \n\n\\textbf{Methods:} We evaluate ten LLMs on 640 physician-verified misinformation posts from a prior mixed-methods study (March 2022-October 2023). Each model issues a five-level predicted verdict (False / Likely-False / Ambiguous / Likely-True / True) under five prompting strategies (no-role; public-health expert; respiratory specialist; public-health expert + source/date context; respiratory specialist + source/date context). A single Qwen judge (\\texttt{qwen-turbo-latest}) maps model responses to one of the five labels. We report strict accuracy (credit only False), lenient accuracy (credit False or Likely-False), ambiguity rate (Ambiguous), error rate (Likely-True / True), and a composite score. \n\n\\textbf{Results:} Across all experiments, the average lenient accuracy was 61.2\\%, with a low overall ambiguity rate (<2\\%). Performance was highly model-dependent: the top-performing configuration achieved approximately 90\\% lenient accuracy, while more conservative models incorrectly accepted over 50\\% of false posts. Counterintuitively, prompting with expert personas and contextual details did not uniformly improve performance and, in many cases, reduced the models' flagging rates.\n\n\\textbf{Contributions:} (1) An empirical, multi-LLM, multi-prompt evaluation on a previously established Chinese COVID-19 misinformation corpus. (2) A systematic comparison of five prompt strategies, quantifying how adding source/date context tends to reduce flagging on this all-misinformation benchmark while modestly lowering ambiguity. (3) Evidence that persona choice (public-health vs respiratory specialist) is not uniformly beneficial across posts and prompts. (4) A reproducible release (prompts, code, judging templates, redacted logs) to support Chinese-language infodemic monitoring and future replication."
      },
      "pdf": {
        "value": "/pdf/6363a04b09327046415ae6dbebf5fd2d8de4ddb2.pdf"
      },
      "supplementary_material": {
        "value": "/attachment/14560dc58b8073defe30f864282879522c999701.zip"
      },
      "venue": {
        "value": "Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference"
      },
      "_bibtex": {
        "value": "@inproceedings{\nzuo2025multillm,\ntitle={Multi-{LLM} and Multi-Prompt Strategies for {COVID}-19 Infodemic Detection in Chinese Social Media: An Empirical Evaluation},\nauthor={Teng Zuo and Hongwen Lin and Lingfeng He and Hongji Zeng and Lina Tang and Li He and Ning Li},\nbooktitle={Open Conference of AI Agents for Science 2025},\nyear={2025},\nurl={https://openreview.net/forum?id=zYDXTzql3n}\n}"
      },
      "paperhash": {
        "value": "zuo|multillm_and_multiprompt_strategies_for_covid19_infodemic_detection_in_chinese_social_media_an_empirical_evaluation"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit",
      "Agents4Science/2025/Conference/Submission199/-/Camera_Ready"
    ],
    "cdate": 1757931770293,
    "pdate": 1759960940583,
    "odate": 1758112145415,
    "mdate": 1760809339079,
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission199/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "wNPPUiwpAf",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "title": {
        "value": "A descent paper with reasonable research ideas and experiments"
      },
      "summary": {
        "value": "This paper employs many LLMs to automatically identify COVID-19 misinformation in social media content, and studies how different prompting strategies affect performance. It has a comprehensive set of metrics that gradually relax how lenient it is to errors. Across all experiments, the average lenient accuracy was 61.2%. The paper also found that performance was highly model-dependent: the top-performing configuration achieved approximately 90%\n lenient accuracy, while more conservative models incorrectly accepted over 50%\nof false posts. Counterintuitively, prompting with expert personas and contextual details did not uniformly improve performance and, in many cases, reduced the models’ flagging rates."
      },
      "strengths_and_weaknesses": {
        "value": "The research question is reasonable, but not innovative enough. Overall, the idea of this paper is somewhat incremental. It can be viewed as a course-project level application of LLMs, though I do appreciate the comprehensive comparisons using various LLMs and prompting. As a research paper, we would hope to see deeper analysis beyond just observational phenomena. For example, any hypothesis and deeper studies about why prompting does not change much, why results do not change over time, why some strong models like GPT 4o is much worse than QWen.  \n\n\nI liked the way that abstract was written. After reading it, the entire paper's key results/findings are mostly clear already. The explanation for accuracy in \"accuracy (credit only False)\" was not clear.  \n \nThe paper writing is pretty clear overall: succinct language and comprehensive descriptions."
      },
      "quality": {
        "value": 2
      },
      "clarity": {
        "value": 4
      },
      "significance": {
        "value": 2
      },
      "originality": {
        "value": 2
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "overall": {
        "value": 3
      },
      "confidence": {
        "value": 4
      },
      "ethical_concerns": {
        "value": "N/A"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission199/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759636926257,
    "mdate": 1760632194252,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_aA9z"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission199/Reviewer_aA9z"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "pQXuu3MQXn",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a comprehensive evaluation of 10 large language models across 5 prompting strategies for detecting COVID-19 misinformation in Chinese social media posts, using a physician-verified corpus of 640 Weibo posts. The methodology is generally sound, with clear experimental design, appropriate evaluation metrics, and a reasonable LLM-as-judge protocol. However, there are concerns about systematic bias from using a single LLM judge (Qwen), the all-misinformation ground truth limiting evaluation scope, and the lack of statistical significance testing. The paper is well-structured and clearly written, with strong reproducibility commitments and appropriate ethical considerations. The work is original in its systematic comparison and counterintuitive findings about expert personas, but the core task and methodology follow established patterns. The impact is limited by focus on a single domain, language, and temporal scope, and results may not generalize to more balanced datasets. Strengths include comprehensive evaluation, systematic comparison, reproducibility, and clear presentation. Weaknesses include the single-judge limitation, all-positive corpus, lack of statistical testing, limited generalizability, and superficial temporal analysis. Overall, the paper makes a solid empirical contribution with valuable insights, but methodological limitations and narrow scope limit its impact."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission199/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775755188,
    "mdate": 1760632192658,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "o65wy51sFU",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Evaluation uses an all-misinformation (one-class) test set, precluding measurement of specificity or balanced detection performance; claims of being ‘above chance’ should be tempered and compared against an ‘always False’ baseline.\n- Single LLM-as-judge without reported human agreement metrics or multi-judge consensus; ‘calibration against a human gold standard’ is mentioned but not quantified.\n- Stochastic generation (temperature=0.5) with one sample per input; no repeated runs, no confidence intervals, and no statistical significance testing reported.\n- Composite score formula printed unclearly and weighting scheme (2/1/−1/−2) is arbitrary without justification or sensitivity analysis.\n- Provider safety settings left at defaults and may differ across models; potential confounding not controlled.\n- Minor formal inconsistencies: cross-references (e.g., limitations in ‘Section 6’), citing per-model drops not visible in tables, and terminology that may confuse given the one-class setup.\n- Quarterly trend analysis lacks per-quarter sample sizes, making interpretation of temporal effects unclear.\n- Judge determinism claim is overstated; sampling-based decoding with temperature>0 is not strictly deterministic.\n- Choice to use an LLM judge to map outputs rather than enforcing structured outputs (e.g., JSON with constrained labels) introduces avoidable labeling noise."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776830227,
    "mdate": 1760639940691,
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "OQOmkAZRLW",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents a comprehensive empirical evaluation of ten large language models (LLMs) on the task of detecting COVID-19 misinformation in Chinese social media. The authors leverage a pre-existing, physician-verified dataset of 640 misinformation posts and systematically test five different prompting strategies. The study is well-designed, the execution is thorough, and the results are presented with exceptional clarity. The paper is a model for how to conduct rigorous empirical research on LLM capabilities in a real-world, high-stakes domain.\n\nQuality:\nThe technical quality of this work is very high. The experimental design is sound, testing a diverse set of 10 models against 5 well-motivated prompt strategies, resulting in a large-scale analysis of 32,000 outputs. The choice to use a physician-verified dataset provides a strong foundation for the study's claims. The methodology of using an LLM-as-a-judge is a pragmatic approach for a study of this scale, and the authors are commendably transparent about its potential limitations, taking appropriate steps like using a fixed judge and deterministic decoding. The defined evaluation metrics, including the novel composite score, are well-suited for the task and provide a nuanced view of model performance. The claims made in the abstract and introduction are robustly supported by the data presented in the clear and informative figures and tables. The authors are honest and upfront about the limitations of their work, which strengthens the credibility of their findings.\n\nClarity:\nThe paper is exceptionally well-written and organized. The narrative flows logically from the motivation and research questions to the methodology, results, and implications. The abstract provides a concise and accurate summary of the work. Figures and tables are of high quality, particularly the heatmaps in Figure 2, which offer a comprehensive overview of the results across all conditions. The methodology is described in sufficient detail to understand the experimental setup clearly.\n\nSignificance:\nThe paper's contribution is highly significant. Misinformation is a critical societal problem, and understanding the capabilities and failure modes of LLMs for this task, particularly in non-English contexts like Chinese social media, is of paramount importance. The key findings are impactful:\n1.  The large performance gap between models, with Chinese-centric models showing a strong advantage, is a crucial data point for practitioners.\n2.  The counter-intuitive result that adding expert personas and contextual details can *reduce* flagging accuracy on an all-misinformation corpus is a profound insight for prompt engineering. It challenges the simplistic assumption that \"more context is always better\" and highlights the need for careful, task-aware prompt design.\nThis work will undoubtedly be a valuable resource for researchers and practitioners in content moderation, public health, and applied AI.\n\nOriginality:\nWhile the paper follows an established paradigm of benchmarking LLMs, its originality lies in the scale of the evaluation, the specific focus on a non-English and high-impact domain, and the novelty of its core findings. The systematic investigation of prompting strategies moves beyond a simple leaderboard-style comparison and provides deeper insights into model behavior. The finding that contextual cues may induce a more \"conservative\" or \"nuanced\" stance in LLMs, leading them to incorrectly accept false claims, is a novel and important contribution to the literature on LLM alignment and safety.\n\nReproducibility:\nThe authors have made an exemplary effort to ensure reproducibility. They commit to releasing prompts, code, judging templates, and redacted logs. Key details such as model endpoints, hyperparameters, and the evaluation pipeline are clearly documented in the paper. Their handling of the private dataset (by respecting the original study's protocol) is responsible and appropriate. This commitment significantly increases the value and long-term utility of the work.\n\nEthics and Limitations:\nThe discussion of limitations and ethical considerations is thorough and thoughtful. The authors explicitly address the main weaknesses of their study, including the reliance on a single LLM judge and the use of an all-misinformation dataset (which means the study evaluates recall/false-negatives, not precision/false-positives). The dedicated \"Responsible AI\" and \"Broader Impact\" sections are well-articulated, considering both the positive potential of the work and the negative risks such as censorship and bias, proposing human-in-the-loop systems as a mitigation strategy.\n\nMinor Weaknesses:\n1.  The primary methodological concern is the use of a single LLM as the judge. While the authors acknowledge this, the paper could have been strengthened by including a small-scale human validation study to measure the agreement between the LLM judge and human experts, thereby calibrating the main results.\n2.  The paper reports descriptive statistics but lacks inferential statistical tests (e.g., confidence intervals or significance tests). While many of the observed performance differences are large, formal statistical analysis would add another layer of rigor to the claims.\n\nDespite these minor points, the paper is an outstanding piece of empirical research. It is rigorous, transparent, and impactful. It provides valuable insights into a critical problem and sets a high standard for future work in this area. It is an excellent fit for the Agents4Science conference and represents the kind of high-quality, application-driven research the field needs."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 6
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 6
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission199/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775754850,
    "mdate": 1760632193308,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "JuGDKSyI0a",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "decision": {
        "value": "Accept"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! Congratualations on the acceptance! Please see the reviews below for feedback."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission199/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759948903414,
    "mdate": 1760632283099,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "BuUfzlKiTf",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "This paper presents an empirical evaluation of 10 contemporary LLMs on physician-verified COVID-19 misinformation posts in Chinese (N=640) using five prompt strategies. The study is notable for its substantive experimental scope (32,000 model responses), clear protocols, and focus on a well-scoped, underexplored question—misinformation detection in Chinese. Results are presented with multiple metrics and careful caveats, and the paper is transparent about its limitations. Key findings include that models exceed chance on this all-misinformation corpus, performance varies by model, adding context often reduces flagging, persona effects are inconsistent, and ambiguity is low.\n\nStrengths include the breadth of the evaluation, clear methodology, and honest discussion of limitations. Weaknesses are significant: reliance on a single LLM-as-judge without reporting human agreement metrics, evaluation only on misinformation posts (precluding specificity/false positive assessment), lack of inferential statistics or uncertainty quantification, ad hoc composite score weighting, potential judge bias, and limited qualitative error analysis. The paper is clearly written, with well-presented visuals and appropriate citations. Its significance is moderate, as it addresses a gap but is limited by the evaluation design. Originality lies in the Chinese-language focus and systematic cross-prompt analysis, but the contribution is incremental. Reproducibility is above average, though limited by privacy and lack of judge–human calibration details.\n\nActionable suggestions include: reporting human–judge agreement, adding a balanced set of true posts, providing uncertainty estimates, sensitivity analysis for scoring, mitigating judge bias, including qualitative examples, and expanding ablations.\n\nOverall, this is a well-executed empirical study with useful insights for the community, but its impact is limited by methodological constraints. I lean toward a borderline accept (score: 4): the work is careful and transparent, but falls short of higher rigor due to evaluation design limitations."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 4
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 4
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission199/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775754291,
    "mdate": 1760632193715,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "ACLW2rKpe1",
    "forum": "zYDXTzql3n",
    "replyto": "zYDXTzql3n",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nNo hallucinated references detected."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777826402,
    "mdate": 1760639939941,
    "signatures": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission199/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]