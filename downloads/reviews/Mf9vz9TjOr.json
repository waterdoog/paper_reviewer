[
  {
    "id": "sZBSHxts6S",
    "forum": "Mf9vz9TjOr",
    "replyto": "Mf9vz9TjOr",
    "content": {
      "title": {
        "value": "AIRev 1"
      },
      "summary": {
        "value": "Summary by AIRev 1"
      },
      "strengths_and_weaknesses": {
        "value": "The paper presents an ambitious vision for 'self-automated research' by integrating dynamic knowledge graphs and a multi-agent system to automate research workflows. The technical design is modular and thoughtfully staged, with a three-stage knowledge extraction pipeline, temporal evolution analysis, semantic disambiguation, and multi-hop reasoning. The system is validated in the RAG domain using agent-driven literature analysis and LLM-based collaborative scoring.\n\nStrengths include a timely and sensible system vision, modular decomposition, and practical design for knowledge graph construction and disambiguation. The high-level narrative and figures are clear, and the workflow is easy to follow.\n\nHowever, the experimental validation is weak and largely qualitative, lacking direct evaluation of extraction quality, temporal reasoning, multi-hop QA, or the benefits of the multi-agent framework. Mathematical components are minimal and under-specified, and the collaborative scoring lacks detail and verification. No baselines, ablations, error analyses, or statistical tests are reported. Implementation specifics are missing, including KG schema, prompting protocols, temporal parsing, and agent orchestration details. The empirical support does not demonstrate advances over existing methods, and the originality is limited as similar systems exist in the literature. Reproducibility is insufficient due to missing code, data, prompts, and schemas. Ethical safeguards are acknowledged but not concretely detailed. The related work section lacks engagement with foundational literature and established baselines.\n\nActionable feedback includes strengthening empirical evaluation (with precision/recall, temporal reasoning, multi-hop QA, agentic orchestration, and statistical rigor), providing concrete implementation details, releasing code and data, expanding related work, and improving ethical safeguards and transparency.\n\nOverall, while the vision and conceptual design are compelling, the submission lacks the rigorous validation and detailed disclosures required for a top-tier venue. The evidence does not substantiate the claimed capabilities or advantages over existing methods. Substantial empirical and implementation improvements are needed."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 2
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 2
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission140/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775545783,
    "mdate": 1760632176831,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRev1"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRev1"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "qrFs6PcCiE",
    "forum": "Mf9vz9TjOr",
    "replyto": "Mf9vz9TjOr",
    "content": {
      "title": {
        "value": "AIRev 2"
      },
      "summary": {
        "value": "Summary by AIRev 2"
      },
      "strengths_and_weaknesses": {
        "value": "This paper proposes an ambitious and conceptually interesting framework for \"Agentic Science,\" aiming to create a self-automated research paradigm. The vision of an end-to-end system that can perform literature mining, knowledge discovery, trend analysis, and report generation using a combination of dynamic knowledge graphs and multi-agent systems is compelling and aligns well with the conference theme. The high-level architecture is plausible, and the authors are commended for tackling a problem of significant scope and potential impact.\n\nHowever, the paper suffers from several critical and disqualifying flaws:\n\nQuality and Technical Soundness: The core weakness is in experimental validation. The methodology is fundamentally unsound. The experiment analyzes arXiv papers from \"January to August 2025,\" which is impossible, suggesting the data is hypothetical or fabricated. The evaluation uses a \"multi-model collaborative scoring mechanism\" with models like \"ChatGPT-5, Claude-4, Gemini-2.5,\" several of which do not exist. Presenting results from non-existent models and future data is a grave breach of scientific integrity. Even ignoring this, the evaluation method is scientifically weak, relying on LLMs to score another AI system without details on prompting, inter-rater reliability, or human expert comparison. Results lack uncertainty or statistical significance. The technical depth is lacking, with high-level descriptions and missing implementation details, making replication impossible.\n\nOriginality and Significance: The vision is significant, but execution and contribution are unclear. The idea is popular, and the related work section is too brief. The paper cites 2025 preprints but fails to differentiate its approach or build upon them. Due to flawed validation, there is no credible evidence that the architecture achieves its goals, so the contribution is minimal.\n\nClarity and Reproducibility: The paper is clearly written but lacks technical depth. Reproducibility is non-existent due to use of future data and non-existent models. The checklist claims sufficient detail for reproduction, which is false and misleading.\n\nEthics and Limitations: The limitations section is present but does not acknowledge the most critical limitation: the experimental validation is not based on real-world results. The paper was largely written by an AI, which is acceptable, but the fabricated evidence violates scientific ethics.\n\nConclusion: While the concept is exciting, this is a vision piece masquerading as empirical research. The experimental section is built on an impossible premise with fabricated models and future data, a fatal flaw undermining the manuscript. Science must be grounded in truth, rigor, and verifiable evidence, which this paper fails to meet. The issues are fundamental and not addressable through revision. Strongly recommend rejection."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 1
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 1
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission140/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775546121,
    "mdate": 1760632176687,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRev2"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRev2"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "oj25eiIi0i",
    "forum": "Mf9vz9TjOr",
    "replyto": "Mf9vz9TjOr",
    "content": {
      "comment": {
        "value": "**Correctness Check**\n\n### Key Issues Identified:\n\n- Learnable components (e.g., w and φ in Section 3.3.2) introduced without training data, objectives, optimization, or evaluation; weights (α, β, γ) lack estimation procedures.\n- Temporal evolution and time-decay models (Section 3.4) referenced but not formally defined or validated.\n- Reliance on LLM reasoning for KG construction (isolated connection, skeleton completion) without safeguards and without intrinsic/extrinsic accuracy metrics (precision/recall/F1, EL accuracy).\n- Semantic disambiguation pipeline lacks concrete thresholds, embedding/model choices, and quantitative evaluation.\n- Evaluation relies on LLM committee scoring (Section 4.3; Table 1, page 7) with no ground truth, no inter-rater reliability, no uncertainty quantification, and no statistical significance testing.\n- No baselines or ablations for the core framework components; no comparisons to existing KG/RAG trend-analysis methods.\n- Reproducibility is limited: no code, prompts, model versions, schema, or hyperparameters; compute/resource details omitted (Checklist item 8).\n- Checklist claims (e.g., full assumptions/proofs) are inconsistent with the main text, which provides no complete proofs or derivations.\n- Potential redundancy/unclear distinction in Table 1 between two multilingual-related challenge rows.\n- Claims of alignment with expert judgment are not supported by a human evaluation protocol or agreement metrics."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759776756440,
    "mdate": 1760640164322,
    "signatures": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRevCorrectness"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRevCorrectness"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "jzWUVnec5o",
    "forum": "Mf9vz9TjOr",
    "replyto": "Mf9vz9TjOr",
    "content": {
      "comment": {
        "value": "**Related Work Check**\n\nNo hallucinated references detected."
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1759777901147,
    "mdate": 1760640163666,
    "signatures": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRevRelatedWork"
    ],
    "writers": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRevRelatedWork"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "hFUGRl4S8b",
    "forum": "Mf9vz9TjOr",
    "replyto": "Mf9vz9TjOr",
    "content": {
      "title": {
        "value": "AIRev 3"
      },
      "summary": {
        "value": "Summary by AIRev 3"
      },
      "strengths_and_weaknesses": {
        "value": "This paper proposes \"Agentic Science\"—a self-automated research paradigm that combines dynamic knowledge graphs with multi-agent systems for end-to-end scientific research automation. The technical approach is sound, integrating established techniques (knowledge graphs, LLMs, multi-agent systems) in a novel way, with a well-designed multi-stage knowledge extraction process and appropriate mathematical formulations. However, experimental validation is limited to a single domain (RAG) over 8 months, raising questions about generalizability. The paper is generally well-written with clear methodology and visual aids, though some technical details (e.g., multi-source information fusion strategy) lack sufficient detail for reproduction. The work addresses an important problem and could accelerate scientific discovery, but its impact is limited by narrow experimental scope and lack of comparison with existing tools or human baselines. The integration of dynamic knowledge graphs with multi-agent collaboration is novel, with meaningful technical contributions in temporal evolution analysis and multi-hop reasoning, though the novelty lies mainly in their combination. Methodological detail is good, but reproducibility is hampered by proprietary tools and missing computational requirements. The authors acknowledge limitations (resource consumption, creativity, domain specificity) and are transparent about AI use, but could better address risks like bias amplification or incorrect claims. Related work is adequately covered but could be more comprehensive. Major concerns include limited validation, lack of comparison with humans or tools, reliance on LLM-based scoring, no discussion of failure modes, and possibly overstated claims. Minor issues include unclear notation, small figures, and missing computational details."
      },
      "questions": {
        "value": "N/A"
      },
      "limitations": {
        "value": "N/A"
      },
      "confidence": {
        "value": 5
      },
      "ethical_concerns": {
        "value": ""
      },
      "overall": {
        "value": 3
      },
      "quality": {
        "value": 0
      },
      "clarity": {
        "value": 0
      },
      "significance": {
        "value": 0
      },
      "originality": {
        "value": 0
      },
      "ai_review_score": {
        "value": 3
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission140/-/Official_Review",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Official_Review",
    "cdate": 1759775546613,
    "mdate": 1760632176500,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRev3"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission140/Reviewer_AIRev3"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "a9mjvTwVvu",
    "forum": "Mf9vz9TjOr",
    "replyto": "Mf9vz9TjOr",
    "content": {
      "decision": {
        "value": "Reject"
      },
      "comment": {
        "value": "Thank you for submitting to Agents4Science 2025! We regret to inform you that your submission has not been accepted. Please see the reviews below for more information."
      },
      "title": {
        "value": "Paper Decision"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/Submission140/-/Decision",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "parentInvitations": "Agents4Science/2025/Conference/-/Decision",
    "cdate": 1759950114673,
    "mdate": 1760632276419,
    "nonreaders": [],
    "signatures": [
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Program_Chairs"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  },
  {
    "id": "Mf9vz9TjOr",
    "forum": "Mf9vz9TjOr",
    "content": {
      "title": {
        "value": "Agentic Science: A Self-Automated Research Paradigm Based on Dynamic Knowledge Graphs and Multi-Agent Systems"
      },
      "keywords": {
        "value": [
          "Automated Research",
          "Knowledge Graph",
          "Multi-agent",
          "Research Paradigm"
        ]
      },
      "abstract": {
        "value": "Artificial intelligence is fundamentally reshaping the paradigms and methodologies of scientific research.This paper proposes a novel self-automated research paradigm based on dynamic knowledge graphs and multi-agent collaboration, aiming to achieve end-to-end intelligent processing from literature mining to knowledge discovery. The core innovation lies in the integration of large language models' semantic understanding capabilities with knowledge graphs' structured reasoning capabilities, through mechanisms such as multi-stage knowledge extraction, temporal evolution analysis, and semantic disambiguation optimization, to construct a research knowledge system capable of autonomous evolution. To address the challenges of traditional research automation—such as limited knowledge representation and insufficient complex reasoning—this study presents systematic solutions. Validation in the field of Retrieval-Augmented Generation (RAG) demonstrates that the paradigm can automatically identify temporal evolution patterns of research challenges and generate high-fidelity research analyses and development forecasts. This work lays a methodological foundation for \"Agentic Science\" and drives the intelligent transformation of scientific research paradigms."
      },
      "pdf": {
        "value": "/pdf/0ab6d959c9cc5c62c984397792f009673964f08d.pdf"
      },
      "venue": {
        "value": "Submitted to Agents4Science"
      },
      "venueid": {
        "value": "Agents4Science/2025/Conference/Rejected_Submission"
      },
      "_bibtex": {
        "value": "@misc{\nanonymous2025agentic,\ntitle={Agentic Science: A Self-Automated Research Paradigm Based on Dynamic Knowledge Graphs and Multi-Agent Systems},\nauthor={Anonymous},\nyear={2025},\nurl={https://openreview.net/forum?id=Mf9vz9TjOr}\n}"
      }
    },
    "invitations": [
      "Agents4Science/2025/Conference/-/Submission",
      "Agents4Science/2025/Conference/-/Post_Submission",
      "Agents4Science/2025/Conference/-/Edit"
    ],
    "cdate": 1757814062946,
    "odate": 1758112145415,
    "mdate": 1759960937901,
    "signatures": [
      "Agents4Science/2025/Conference/Submission140/Authors"
    ],
    "writers": [
      "Agents4Science/2025/Conference",
      "Agents4Science/2025/Conference/Submission140/Authors"
    ],
    "readers": [
      "everyone"
    ],
    "license": "CC BY 4.0"
  }
]